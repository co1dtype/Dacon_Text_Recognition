{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e51c474-2e9b-4ebe-9db7-e7e871d5a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"5\" # 학습 환경에 따라 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chosun\\anaconda3\\envs\\ML\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import resnet152\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df3f2-62d0-4499-a46e-47d01699def0",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3367399-9798-4e38-967b-fd2320b9a2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_HEIGHT_SIZE':32,\n",
    "    'IMG_WIDTH_SIZE':128,\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':128,\n",
    "    'NUM_WORKERS':20, # 본인의 GPU, CPU 환경에 맞게 설정\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab024f3-260b-45e8-8fef-9dc91cac6593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "\n",
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Load & Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62c78cd-4f40-4e98-b8a6-1b6f1d906b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb8dc411-4221-4ca8-a887-ecd535506519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76888"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1e9b48-74b0-4467-bd23-5e67b4ad0764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23703"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제공된 학습데이터 중 1글자 샘플들의 단어사전이 학습/테스트 데이터의 모든 글자를 담고 있으므로 학습 데이터로 우선 배치\n",
    "df['len'] = df['label'].str.len()\n",
    "train_v1 = df[df['len']==1]\n",
    "len(train_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ef1d10-8f7d-4807-aec4-728bf08a2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이describe2글자 이상의 샘플들에 대해서 단어길이를 고려하여 Train (80%) / Validation (20%) 분할\n",
    "df = df[df['len']>1]\n",
    "train_v2, val, _, _ = train_test_split(df, df['len'], test_size=0.1, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b53142c6-4d17-44a1-a0c1-c23ee7f83f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71569 5319\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로 우선 배치한 1글자 샘플들과 분할된 2글자 이상의 학습 샘플을 concat하여 최종 학습 데이터로 사용\n",
    "train = pd.concat([train_v1, train_v2])\n",
    "#val = pd.concat([val_v1, val_v2])\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce77b1f2-922f-47a4-b969-f758733ef5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>len_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>25756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len  len_count\n",
       "0    2      25756\n",
       "1    1      23703\n",
       "2    3      12134\n",
       "3    4       9036\n",
       "4    5        919\n",
       "5    6         21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_len_count = pd.DataFrame(train['len'].value_counts())\n",
    "df_len_count.reset_index(inplace=True)\n",
    "df_len_count.columns = ['len', 'len_count']\n",
    "\n",
    "display(df_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4977542-4e00-4831-b912-99d5fc5ef26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='len', ylabel='len_count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(data=df_len_count, x='len', y='len_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43b671-f8a5-403a-b2aa-7f779f6fd85a",
   "metadata": {},
   "source": [
    "## Get Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a384ae9-fc56-4660-96c2-0d424f64ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터로부터 단어 사전(Vocabulary) 구축\n",
    "train_gt = [gt for gt in train['label']]\n",
    "train_gt = \"\".join(train_gt)\n",
    "letters = sorted(list(set(list(train_gt))))\n",
    "print(len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be0b36a1-bc45-4e2c-9f0a-e8f3e845299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"-\"] + letters\n",
    "print(len(vocabulary))\n",
    "idx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
    "char2idx = {v:k for k,v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_path_list[index]).convert('RGB')\n",
    "        \n",
    "        if self.train_mode:\n",
    "            image = self.train_transform(image)\n",
    "        else:\n",
    "            image = self.test_transform(image)\n",
    "            \n",
    "        if self.label_list is not None:\n",
    "            text = self.label_list[index]\n",
    "            return image, text\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    # Image Augmentation\n",
    "    def train_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.RandomResizedCrop((CFG['IMG_HEIGHT_SIZE'], CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ColorJitter(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)\n",
    "    \n",
    "    def val_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)\n",
    "    \n",
    "    def test_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ColorJitter(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['img_path'].values, train['label'].values, True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label'].values, False)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eccac93-a67b-4b45-908a-2283131fd5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 128]) ('초보자', '양심', '뼙', '정신적', '정도', '소화', '일정하다', '찹', '지방', '취소', '울리다', '이래', '발휘하다', '금', '찍히다', '핀', '규', '배', '맛보다', '깔끔하다', '가져가다', '할아버지', '넉', '소규모', '손톱', '헐', '날카롭다', '쐤', '꾜', '온', '메우다', '체계적', '바치다', '학위', '대합실', '체온', '연기', '판', '칠십', '줄다', '고등학교', '단점', '떨어지다', '빗방울', '인간적', '온종일', '실현되다', '절', '가운데', '년', '이빨', '낢', '인천', '색다르다', '소문', '전문직', '낯설다', '투표', '살아나다', '넓', '이용자', '법', '닢', '안다', '연구실', '반장', '지나가다', '깨다', '기억', '숫자', '회견', '포장', '단지', '온갖', '볏', '쉬다', '퍼지다', '지', '륫', '술', '구분되다', '급증하다', '아래쪽', '행복', '북', '차다', '귿', '예습', '평양', '곈', '전시되다', '스케줄', '뎨', '방법', '지다', '홍', '쉼', '터미널', '흥', '특', '안', '그만', '엑', '일기', '급격히', '숫', '올', '그', '전하다', '쁠', '어쩜', '투표', '초등학교', '육', '세', '시작되다', '어떡하다', '바', '소극적', '삼', '밝히다', '여', '팸', '출근', '년', '한여름', '각', '귀국하다')\n"
     ]
    }
   ],
   "source": [
    "image_batch, text_batch = iter(train_loader).next()\n",
    "print(image_batch.size(), text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb47207f-0aa0-4434-82f7-fe442c13558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12627115b50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = image_batch[17]\n",
    "img = img.cpu().numpy().T\n",
    "img = np.swapaxes(img, 0, 1)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ef9ef9-21c1-4b7e-b697-e067ad8be3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Code\\\\Lab\\\\Text_Recognition'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f68208-2855-4d4e-bce2-bef1eea85245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"./train/TRAIN_13845.png\")\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83d258f1-96a8-435b-b23a-f93436674f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 128]) ('지점', '한때', '현대인', '띄우다', '지도자', '자신', '여자', '숨기다', '신고하다', '근거하다', '공연되다', '바람', '언어', '대도시', '아쉬움', '선수', '나가다', '미소', '일요일', '전통', '그루', '수염', '안다', '아니', '문장', '화면', '차다', '불법', '소프트웨어', '부러워하다', '코스', '가만있다', '보조', '살리다', '대규모', '겪다', '전부', '닦다', '예술적', '필연적', '수면', '열정', '손톱', '숙제', '가끔', '공연', '완전', '강도', '두세', '줄다', '기혼', '운동복', '힘껏', '거리', '믿다', '고려', '깔다', '독하다', '손쉽다', '고등학교', '운전자', '출판', '일주일', '뒷골목', '인하다', '성적', '만두', '관계없이', '발생하다', '업무', '배경', '탁월하다', '연구실', '가다', '성공적', '간부', '자매', '햇살', '의복', '소리치다', '멀어지다', '연기되다', '칼국수', '아래층', '바늘', '머릿속', '이상', '굽히다', '화면', '신화', '강남', '취직', '동전', '외국어', '떨어지다', '러시아', '문제', '오페라', '오늘', '기름', '축구공', '수출', '끄다', '무엇', '생각되다', '당시', '엉망', '으레', '지도', '도로', '회색', '아니하다', '체중', '주먹', '마련', '소비하다', '참여하다', '이르다', '안경', '목숨', '씩씩하다', '인구', '일주일', '창고', '요리', '운동화', '고려하다', '오다')\n"
     ]
    }
   ],
   "source": [
    "image_batch, text_batch = iter(val_loader).next()\n",
    "print(image_batch.size(), text_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d8aa8-d591-40e1-9160-63b4519c8c72",
   "metadata": {},
   "source": [
    "## Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c837270-76be-4bc7-9c01-333ced3c72e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread(\"train/TRAIN_00000.png\", cv2.IMREAD_COLOR)\n",
    "dst = cv2.fastNlMeansDenoisingColored(img,None,10,10,7,21)\n",
    "plt.subplot(121),plt.imshow(img)\n",
    "plt.subplot(122),plt.imshow(dst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionModel(nn.Module):\n",
    "    def __init__(self, num_chars=len(char2idx), rnn_hidden_size=256):\n",
    "        super(RecognitionModel, self).__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "        resnet = resnet152(pretrained=True)\n",
    "        resnet_modules = list(resnet.children())[:-3]\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            *resnet_modules,\n",
    "            nn.Conv2d(1024, 1024, kernel_size=(3,6), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.linear1 = nn.Linear(2048, rnn_hidden_size)\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_size=rnn_hidden_size, \n",
    "                            hidden_size=rnn_hidden_size,\n",
    "                            bidirectional=True, \n",
    "                            batch_first=True)\n",
    "        self.linear2 = nn.Linear(self.rnn_hidden_size*2, num_chars)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.feature_extract(x) # [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2) # [batch_size, width, channels, height]\n",
    "         \n",
    "        batch_size = x.size(0)\n",
    "        T = x.size(1)\n",
    "        x = x.view(batch_size, T, -1) # [batch_size, T==width, num_features==channels*height]\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # RNN\n",
    "        x, hidden = self.rnn(x)\n",
    "        \n",
    "        output = self.linear2(x)\n",
    "        output = output.permute(1, 0, 2) # [T==10, batch_size, num_classes==num_features]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f5723-1137-4b6d-be4a-47b1c99e2744",
   "metadata": {},
   "source": [
    "## Define CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a34330-d5cd-4703-8930-45bd238e04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=0) # idx 0 : '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dda15898-58b3-4431-8aa2-09cc2ea1c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_batch(text_batch):\n",
    "    text_batch_targets_lens = [len(text) for text in text_batch]\n",
    "    text_batch_targets_lens = torch.IntTensor(text_batch_targets_lens)\n",
    "    \n",
    "    text_batch_concat = \"\".join(text_batch)\n",
    "    text_batch_targets = [char2idx[c] for c in text_batch_concat]\n",
    "    text_batch_targets = torch.IntTensor(text_batch_targets)\n",
    "    \n",
    "    return text_batch_targets, text_batch_targets_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2bddfe3-30b0-42a2-8954-a8a19457be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(text_batch, text_batch_logits):\n",
    "    \"\"\"\n",
    "    text_batch: list of strings of length equal to batch size\n",
    "    text_batch_logits: Tensor of size([T, batch_size, num_classes])\n",
    "    \"\"\"\n",
    "    text_batch_logps = F.log_softmax(text_batch_logits, 2) # [T, batch_size, num_classes]  \n",
    "    text_batch_logps_lens = torch.full(size=(text_batch_logps.size(1),), \n",
    "                                       fill_value=text_batch_logps.size(0), \n",
    "                                       dtype=torch.int32).to(device) # [batch_size] \n",
    "\n",
    "    text_batch_targets, text_batch_targets_lens = encode_text_batch(text_batch)\n",
    "    loss = criterion(text_batch_logps, text_batch_targets, text_batch_logps_lens, text_batch_targets_lens)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9698d802-1708-4ede-b692-c863c562ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text):\n",
    "    if len(text) > 1:\n",
    "        letters = [text[0]] + [letter for idx, letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n",
    "    elif len(text) == 1:\n",
    "        letters = [text[0]]\n",
    "    else:\n",
    "        return \"\"\n",
    "    return \"\".join(letters)\n",
    "\n",
    "def correct_prediction(word):\n",
    "    parts = word.split(\"-\")\n",
    "    parts = [remove_duplicates(part) for part in parts]\n",
    "    corrected_word = \"\".join(parts)\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69749109-3586-4b5a-a04b-188f24a35031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(text_batch, text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.T # [batch_size, T]\n",
    "\n",
    "    check = []; cnt = 0\n",
    "    for text_tokens, answer in zip(text_batch_tokens, text_batch):\n",
    "        text = [idx2char[int(idx)] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        \n",
    "        check.append(1 if correct_prediction(text) == answer else 0)\n",
    "            \n",
    "    return check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857332e-6406-4125-a143-8d71a02a2989",
   "metadata": {},
   "source": [
    "## Training Recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58d16bd2-980b-4f3a-a317-ebc3c4dd53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_epoch = 1\n",
    "recode_df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss',\n",
    "                                 'train_acc', 'val_acc', 'best_loss',\n",
    "                                  'best_acc'])\n",
    "dir_name = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08625080-7c35-4730-9103-b25ae943fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recode(epoch, model, train_loss, val_loss, train_acc, val_acc, best_loss, best_acc, model_save=False):\n",
    "    global recode_df, fig, axes\n",
    "    \n",
    "    # Data Write\n",
    "    os.makedirs(f\"./result/{dir_name}\", exist_ok=True)\n",
    "    new_data = {\n",
    "        \"epoch\" : epoch,\n",
    "        \"train_loss\" : train_loss,\n",
    "        \"val_loss\" : val_loss,\n",
    "        \"train_acc\" : train_acc,\n",
    "        \"val_acc\" : val_acc,\n",
    "        \"best_loss\" : best_loss,\n",
    "        \"best_acc\" : best_acc\n",
    "    }\n",
    "    \n",
    "    recode_df = recode_df.append(new_data,  ignore_index=True)\n",
    "    recode_df.to_csv(f'./result/{dir_name}/recode.csv', index=False)\n",
    "    \n",
    "    \n",
    "    # Data Visualization \n",
    "    os.makedirs(f\"./result/{dir_name}/plot\", exist_ok=True)\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.plot(recode_df['train_loss'].to_list(), label=\"train loss\")\n",
    "    ax1.plot(recode_df['val_loss'].to_list(), label=\"val loss\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.plot(recode_df['train_acc'], label=\"train Accuracy\")\n",
    "    ax2.plot(recode_df['val_acc'], label=\"val Accuracy\")\n",
    "    ax2.legend()\n",
    "    #plt.close(fig)\n",
    "    #plt.ioff()\n",
    "    \n",
    "    if os.path.isfile(f\"./result/{dir_name}/plot/loss_and_acc.png\"):\n",
    "        os.remove(f\"./result/{dir_name}/plot/loss_and_acc.png\")\n",
    "    plt.savefig(f\"./result/{dir_name}/plot/loss_and_acc.png\")\n",
    "    \n",
    "    # Write Hyperparameter\n",
    "    if epoch == 1:\n",
    "        with open(f\"./result/{dir_name}/train.txt\",'w',encoding='UTF-8') as f:\n",
    "            for key, values in CFG.items():\n",
    "                f.write(f\"{key} {values}\" + \"\\n\")\n",
    "         \n",
    "    if model_save:\n",
    "        torch.save(model, f\"./result/{dir_name}/model.pt\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    global current_epoch\n",
    "    model.to(device)\n",
    "    \n",
    "    best_model = None\n",
    "    early_stop = 5\n",
    "    \n",
    "    if current_epoch != 1:\n",
    "        tmp = pd.read_csv(f'./result/{dir_name}/recode.csv')\n",
    "        best_acc = tmp['best_loss'][current_epoch-2]\n",
    "        best_loss = tmp['best_acc'][current_epoch-2]\n",
    "        if os.path.isfile(f\"./result/{dir_name}/model.pt\"):\n",
    "            model = torch.load(f\"./result/{dir_name}/model.pt\", map_location= device)\n",
    "    else:\n",
    "        best_acc = -1\n",
    "        best_loss = 1e9\n",
    "        \n",
    "        \n",
    "    for epoch in range(current_epoch, current_epoch + CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        model_save = False\n",
    "        for image_batch, text_batch in tqdm(iter(train_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                text_batch_logits = model(image_batch)\n",
    "                loss = compute_loss(text_batch, text_batch_logits)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_acc += compute_acc(text_batch, text_batch_logits)\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        _train_loss = np.mean(train_loss)\n",
    "        \n",
    "        \n",
    "        _train_acc = 100. * sum(train_acc) / len(train_loader.dataset)\n",
    "        _val_loss, _val_acc = validation(model, val_loader, device)\n",
    "        \n",
    "        print(f'Epoch : [{epoch}] Train CTC Loss : [{_train_loss:.5f}] Val CTC Loss : [{_val_loss:.5f}]')\n",
    "        print(f'Epoch : [{epoch}] Train Accuracy : [{_train_acc:.5f}] Val Accuracy : [{_val_acc:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "        \n",
    "        if best_loss > _val_loss or best_acc < _val_acc:\n",
    "            early_stop += 1\n",
    "        \n",
    "        if best_loss > _val_loss:\n",
    "            best_loss = _val_loss\n",
    "            \n",
    "        if best_acc < _val_acc:\n",
    "            best_acc = _val_acc\n",
    "            best_model = model\n",
    "            model_save = True\n",
    "            \n",
    "        early_stop -= 1\n",
    "        current_epoch += 1\n",
    "        Recode(epoch, model, _train_loss, _val_loss, _train_acc, _val_acc, best_loss, best_acc, model_save)\n",
    "        \n",
    "        if early_stop == 0:\n",
    "            break\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb26e02-35de-418b-8948-7447f5822cfe",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18efe906-aeb7-45a6-8db5-aed806fd7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []; val_acc = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch, text_batch in tqdm(iter(val_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                text_batch_logits = model(image_batch)\n",
    "                loss = compute_loss(text_batch, text_batch_logits)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            val_acc += compute_acc(text_batch, text_batch_logits)\n",
    "    \n",
    "    _val_loss = np.mean(val_loss)\n",
    "    _val_acc = 100. * sum(val_acc) / len(val_loader.dataset)\n",
    "    return _val_loss, _val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99693423-b3d4-4244-8367-73037e8ee710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chosun/.cache\\torch\\hub\\baudm_parseq_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('baudm/parseq', 'parseq', pretrained=True).eval()\n",
    "model.text_embed.embedding = torch.nn.Embedding(2350, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d99e7c7c-afce-4619-b4eb-9963cdc35d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PARSeq(\n",
       "  (encoder): Encoder(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(4, 8), stride=(4, 8))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (cross_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_q): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_c): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): Linear(in_features=384, out_features=95, bias=True)\n",
       "  (text_embed): TokenEmbedding(\n",
       "    (embedding): Embedding(2350, 384)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1dc5865d-c247-4691-bb1f-41d083b0c197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = RecognitionModel()\n",
    "#model.eval()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"], weight_decay=0.3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48f3ae59-ab4e-46a3-8294-9b1b9afef9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [128, 384, 8, 16]          37,248\n",
      "          Identity-2            [128, 128, 384]               0\n",
      "        PatchEmbed-3            [128, 128, 384]               0\n",
      "           Dropout-4            [128, 128, 384]               0\n",
      "          Identity-5            [128, 128, 384]               0\n",
      "         LayerNorm-6            [128, 128, 384]             768\n",
      "            Linear-7           [128, 128, 1152]         443,520\n",
      "           Dropout-8         [128, 6, 128, 128]               0\n",
      "            Linear-9            [128, 128, 384]         147,840\n",
      "          Dropout-10            [128, 128, 384]               0\n",
      "        Attention-11            [128, 128, 384]               0\n",
      "         Identity-12            [128, 128, 384]               0\n",
      "         Identity-13            [128, 128, 384]               0\n",
      "        LayerNorm-14            [128, 128, 384]             768\n",
      "           Linear-15           [128, 128, 1536]         591,360\n",
      "             GELU-16           [128, 128, 1536]               0\n",
      "          Dropout-17           [128, 128, 1536]               0\n",
      "           Linear-18            [128, 128, 384]         590,208\n",
      "          Dropout-19            [128, 128, 384]               0\n",
      "              Mlp-20            [128, 128, 384]               0\n",
      "         Identity-21            [128, 128, 384]               0\n",
      "         Identity-22            [128, 128, 384]               0\n",
      "            Block-23            [128, 128, 384]               0\n",
      "        LayerNorm-24            [128, 128, 384]             768\n",
      "           Linear-25           [128, 128, 1152]         443,520\n",
      "          Dropout-26         [128, 6, 128, 128]               0\n",
      "           Linear-27            [128, 128, 384]         147,840\n",
      "          Dropout-28            [128, 128, 384]               0\n",
      "        Attention-29            [128, 128, 384]               0\n",
      "         Identity-30            [128, 128, 384]               0\n",
      "         Identity-31            [128, 128, 384]               0\n",
      "        LayerNorm-32            [128, 128, 384]             768\n",
      "           Linear-33           [128, 128, 1536]         591,360\n",
      "             GELU-34           [128, 128, 1536]               0\n",
      "          Dropout-35           [128, 128, 1536]               0\n",
      "           Linear-36            [128, 128, 384]         590,208\n",
      "          Dropout-37            [128, 128, 384]               0\n",
      "              Mlp-38            [128, 128, 384]               0\n",
      "         Identity-39            [128, 128, 384]               0\n",
      "         Identity-40            [128, 128, 384]               0\n",
      "            Block-41            [128, 128, 384]               0\n",
      "        LayerNorm-42            [128, 128, 384]             768\n",
      "           Linear-43           [128, 128, 1152]         443,520\n",
      "          Dropout-44         [128, 6, 128, 128]               0\n",
      "           Linear-45            [128, 128, 384]         147,840\n",
      "          Dropout-46            [128, 128, 384]               0\n",
      "        Attention-47            [128, 128, 384]               0\n",
      "         Identity-48            [128, 128, 384]               0\n",
      "         Identity-49            [128, 128, 384]               0\n",
      "        LayerNorm-50            [128, 128, 384]             768\n",
      "           Linear-51           [128, 128, 1536]         591,360\n",
      "             GELU-52           [128, 128, 1536]               0\n",
      "          Dropout-53           [128, 128, 1536]               0\n",
      "           Linear-54            [128, 128, 384]         590,208\n",
      "          Dropout-55            [128, 128, 384]               0\n",
      "              Mlp-56            [128, 128, 384]               0\n",
      "         Identity-57            [128, 128, 384]               0\n",
      "         Identity-58            [128, 128, 384]               0\n",
      "            Block-59            [128, 128, 384]               0\n",
      "        LayerNorm-60            [128, 128, 384]             768\n",
      "           Linear-61           [128, 128, 1152]         443,520\n",
      "          Dropout-62         [128, 6, 128, 128]               0\n",
      "           Linear-63            [128, 128, 384]         147,840\n",
      "          Dropout-64            [128, 128, 384]               0\n",
      "        Attention-65            [128, 128, 384]               0\n",
      "         Identity-66            [128, 128, 384]               0\n",
      "         Identity-67            [128, 128, 384]               0\n",
      "        LayerNorm-68            [128, 128, 384]             768\n",
      "           Linear-69           [128, 128, 1536]         591,360\n",
      "             GELU-70           [128, 128, 1536]               0\n",
      "          Dropout-71           [128, 128, 1536]               0\n",
      "           Linear-72            [128, 128, 384]         590,208\n",
      "          Dropout-73            [128, 128, 384]               0\n",
      "              Mlp-74            [128, 128, 384]               0\n",
      "         Identity-75            [128, 128, 384]               0\n",
      "         Identity-76            [128, 128, 384]               0\n",
      "            Block-77            [128, 128, 384]               0\n",
      "        LayerNorm-78            [128, 128, 384]             768\n",
      "           Linear-79           [128, 128, 1152]         443,520\n",
      "          Dropout-80         [128, 6, 128, 128]               0\n",
      "           Linear-81            [128, 128, 384]         147,840\n",
      "          Dropout-82            [128, 128, 384]               0\n",
      "        Attention-83            [128, 128, 384]               0\n",
      "         Identity-84            [128, 128, 384]               0\n",
      "         Identity-85            [128, 128, 384]               0\n",
      "        LayerNorm-86            [128, 128, 384]             768\n",
      "           Linear-87           [128, 128, 1536]         591,360\n",
      "             GELU-88           [128, 128, 1536]               0\n",
      "          Dropout-89           [128, 128, 1536]               0\n",
      "           Linear-90            [128, 128, 384]         590,208\n",
      "          Dropout-91            [128, 128, 384]               0\n",
      "              Mlp-92            [128, 128, 384]               0\n",
      "         Identity-93            [128, 128, 384]               0\n",
      "         Identity-94            [128, 128, 384]               0\n",
      "            Block-95            [128, 128, 384]               0\n",
      "        LayerNorm-96            [128, 128, 384]             768\n",
      "           Linear-97           [128, 128, 1152]         443,520\n",
      "          Dropout-98         [128, 6, 128, 128]               0\n",
      "           Linear-99            [128, 128, 384]         147,840\n",
      "         Dropout-100            [128, 128, 384]               0\n",
      "       Attention-101            [128, 128, 384]               0\n",
      "        Identity-102            [128, 128, 384]               0\n",
      "        Identity-103            [128, 128, 384]               0\n",
      "       LayerNorm-104            [128, 128, 384]             768\n",
      "          Linear-105           [128, 128, 1536]         591,360\n",
      "            GELU-106           [128, 128, 1536]               0\n",
      "         Dropout-107           [128, 128, 1536]               0\n",
      "          Linear-108            [128, 128, 384]         590,208\n",
      "         Dropout-109            [128, 128, 384]               0\n",
      "             Mlp-110            [128, 128, 384]               0\n",
      "        Identity-111            [128, 128, 384]               0\n",
      "        Identity-112            [128, 128, 384]               0\n",
      "           Block-113            [128, 128, 384]               0\n",
      "       LayerNorm-114            [128, 128, 384]             768\n",
      "          Linear-115           [128, 128, 1152]         443,520\n",
      "         Dropout-116         [128, 6, 128, 128]               0\n",
      "          Linear-117            [128, 128, 384]         147,840\n",
      "         Dropout-118            [128, 128, 384]               0\n",
      "       Attention-119            [128, 128, 384]               0\n",
      "        Identity-120            [128, 128, 384]               0\n",
      "        Identity-121            [128, 128, 384]               0\n",
      "       LayerNorm-122            [128, 128, 384]             768\n",
      "          Linear-123           [128, 128, 1536]         591,360\n",
      "            GELU-124           [128, 128, 1536]               0\n",
      "         Dropout-125           [128, 128, 1536]               0\n",
      "          Linear-126            [128, 128, 384]         590,208\n",
      "         Dropout-127            [128, 128, 384]               0\n",
      "             Mlp-128            [128, 128, 384]               0\n",
      "        Identity-129            [128, 128, 384]               0\n",
      "        Identity-130            [128, 128, 384]               0\n",
      "           Block-131            [128, 128, 384]               0\n",
      "       LayerNorm-132            [128, 128, 384]             768\n",
      "          Linear-133           [128, 128, 1152]         443,520\n",
      "         Dropout-134         [128, 6, 128, 128]               0\n",
      "          Linear-135            [128, 128, 384]         147,840\n",
      "         Dropout-136            [128, 128, 384]               0\n",
      "       Attention-137            [128, 128, 384]               0\n",
      "        Identity-138            [128, 128, 384]               0\n",
      "        Identity-139            [128, 128, 384]               0\n",
      "       LayerNorm-140            [128, 128, 384]             768\n",
      "          Linear-141           [128, 128, 1536]         591,360\n",
      "            GELU-142           [128, 128, 1536]               0\n",
      "         Dropout-143           [128, 128, 1536]               0\n",
      "          Linear-144            [128, 128, 384]         590,208\n",
      "         Dropout-145            [128, 128, 384]               0\n",
      "             Mlp-146            [128, 128, 384]               0\n",
      "        Identity-147            [128, 128, 384]               0\n",
      "        Identity-148            [128, 128, 384]               0\n",
      "           Block-149            [128, 128, 384]               0\n",
      "       LayerNorm-150            [128, 128, 384]             768\n",
      "          Linear-151           [128, 128, 1152]         443,520\n",
      "         Dropout-152         [128, 6, 128, 128]               0\n",
      "          Linear-153            [128, 128, 384]         147,840\n",
      "         Dropout-154            [128, 128, 384]               0\n",
      "       Attention-155            [128, 128, 384]               0\n",
      "        Identity-156            [128, 128, 384]               0\n",
      "        Identity-157            [128, 128, 384]               0\n",
      "       LayerNorm-158            [128, 128, 384]             768\n",
      "          Linear-159           [128, 128, 1536]         591,360\n",
      "            GELU-160           [128, 128, 1536]               0\n",
      "         Dropout-161           [128, 128, 1536]               0\n",
      "          Linear-162            [128, 128, 384]         590,208\n",
      "         Dropout-163            [128, 128, 384]               0\n",
      "             Mlp-164            [128, 128, 384]               0\n",
      "        Identity-165            [128, 128, 384]               0\n",
      "        Identity-166            [128, 128, 384]               0\n",
      "           Block-167            [128, 128, 384]               0\n",
      "       LayerNorm-168            [128, 128, 384]             768\n",
      "          Linear-169           [128, 128, 1152]         443,520\n",
      "         Dropout-170         [128, 6, 128, 128]               0\n",
      "          Linear-171            [128, 128, 384]         147,840\n",
      "         Dropout-172            [128, 128, 384]               0\n",
      "       Attention-173            [128, 128, 384]               0\n",
      "        Identity-174            [128, 128, 384]               0\n",
      "        Identity-175            [128, 128, 384]               0\n",
      "       LayerNorm-176            [128, 128, 384]             768\n",
      "          Linear-177           [128, 128, 1536]         591,360\n",
      "            GELU-178           [128, 128, 1536]               0\n",
      "         Dropout-179           [128, 128, 1536]               0\n",
      "          Linear-180            [128, 128, 384]         590,208\n",
      "         Dropout-181            [128, 128, 384]               0\n",
      "             Mlp-182            [128, 128, 384]               0\n",
      "        Identity-183            [128, 128, 384]               0\n",
      "        Identity-184            [128, 128, 384]               0\n",
      "           Block-185            [128, 128, 384]               0\n",
      "       LayerNorm-186            [128, 128, 384]             768\n",
      "          Linear-187           [128, 128, 1152]         443,520\n",
      "         Dropout-188         [128, 6, 128, 128]               0\n",
      "          Linear-189            [128, 128, 384]         147,840\n",
      "         Dropout-190            [128, 128, 384]               0\n",
      "       Attention-191            [128, 128, 384]               0\n",
      "        Identity-192            [128, 128, 384]               0\n",
      "        Identity-193            [128, 128, 384]               0\n",
      "       LayerNorm-194            [128, 128, 384]             768\n",
      "          Linear-195           [128, 128, 1536]         591,360\n",
      "            GELU-196           [128, 128, 1536]               0\n",
      "         Dropout-197           [128, 128, 1536]               0\n",
      "          Linear-198            [128, 128, 384]         590,208\n",
      "         Dropout-199            [128, 128, 384]               0\n",
      "             Mlp-200            [128, 128, 384]               0\n",
      "        Identity-201            [128, 128, 384]               0\n",
      "        Identity-202            [128, 128, 384]               0\n",
      "           Block-203            [128, 128, 384]               0\n",
      "       LayerNorm-204            [128, 128, 384]             768\n",
      "          Linear-205           [128, 128, 1152]         443,520\n",
      "         Dropout-206         [128, 6, 128, 128]               0\n",
      "          Linear-207            [128, 128, 384]         147,840\n",
      "         Dropout-208            [128, 128, 384]               0\n",
      "       Attention-209            [128, 128, 384]               0\n",
      "        Identity-210            [128, 128, 384]               0\n",
      "        Identity-211            [128, 128, 384]               0\n",
      "       LayerNorm-212            [128, 128, 384]             768\n",
      "          Linear-213           [128, 128, 1536]         591,360\n",
      "            GELU-214           [128, 128, 1536]               0\n",
      "         Dropout-215           [128, 128, 1536]               0\n",
      "          Linear-216            [128, 128, 384]         590,208\n",
      "         Dropout-217            [128, 128, 384]               0\n",
      "             Mlp-218            [128, 128, 384]               0\n",
      "        Identity-219            [128, 128, 384]               0\n",
      "        Identity-220            [128, 128, 384]               0\n",
      "           Block-221            [128, 128, 384]               0\n",
      "       LayerNorm-222            [128, 128, 384]             768\n",
      "         Encoder-223            [128, 128, 384]               0\n",
      "       Embedding-224              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-225              [128, 1, 384]               0\n",
      "       Embedding-226              [128, 0, 384]         902,400\n",
      "  TokenEmbedding-227              [128, 0, 384]               0\n",
      "         Dropout-228              [128, 1, 384]               0\n",
      "         Dropout-229              [128, 1, 384]               0\n",
      "       LayerNorm-230              [128, 1, 384]             768\n",
      "       LayerNorm-231              [128, 1, 384]             768\n",
      "MultiheadAttention-232  [[-1, 1, 384], [-1, 1, 1]]               0\n",
      "         Dropout-233              [128, 1, 384]               0\n",
      "       LayerNorm-234              [128, 1, 384]             768\n",
      "MultiheadAttention-235  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-236              [128, 1, 384]               0\n",
      "       LayerNorm-237              [128, 1, 384]             768\n",
      "          Linear-238             [128, 1, 1536]         591,360\n",
      "         Dropout-239             [128, 1, 1536]               0\n",
      "          Linear-240              [128, 1, 384]         590,208\n",
      "         Dropout-241              [128, 1, 384]               0\n",
      "    DecoderLayer-242  [[-1, 1, 384], [-1, 1, 384]]               0\n",
      "       LayerNorm-243              [128, 1, 384]             768\n",
      "         Decoder-244              [128, 1, 384]               0\n",
      "          Linear-245               [128, 1, 95]          36,575\n",
      "       Embedding-246              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-247              [128, 1, 384]               0\n",
      "       Embedding-248              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-249              [128, 1, 384]               0\n",
      "         Dropout-250              [128, 2, 384]               0\n",
      "         Dropout-251              [128, 1, 384]               0\n",
      "       LayerNorm-252              [128, 1, 384]             768\n",
      "       LayerNorm-253              [128, 2, 384]             768\n",
      "MultiheadAttention-254  [[-1, 1, 384], [-1, 1, 2]]               0\n",
      "         Dropout-255              [128, 1, 384]               0\n",
      "       LayerNorm-256              [128, 1, 384]             768\n",
      "MultiheadAttention-257  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-258              [128, 1, 384]               0\n",
      "       LayerNorm-259              [128, 1, 384]             768\n",
      "          Linear-260             [128, 1, 1536]         591,360\n",
      "         Dropout-261             [128, 1, 1536]               0\n",
      "          Linear-262              [128, 1, 384]         590,208\n",
      "         Dropout-263              [128, 1, 384]               0\n",
      "    DecoderLayer-264  [[-1, 1, 384], [-1, 2, 384]]               0\n",
      "       LayerNorm-265              [128, 1, 384]             768\n",
      "         Decoder-266              [128, 1, 384]               0\n",
      "          Linear-267               [128, 1, 95]          36,575\n",
      "       Embedding-268              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-269              [128, 1, 384]               0\n",
      "       Embedding-270              [128, 2, 384]         902,400\n",
      "  TokenEmbedding-271              [128, 2, 384]               0\n",
      "         Dropout-272              [128, 3, 384]               0\n",
      "         Dropout-273              [128, 1, 384]               0\n",
      "       LayerNorm-274              [128, 1, 384]             768\n",
      "       LayerNorm-275              [128, 3, 384]             768\n",
      "MultiheadAttention-276  [[-1, 1, 384], [-1, 1, 3]]               0\n",
      "         Dropout-277              [128, 1, 384]               0\n",
      "       LayerNorm-278              [128, 1, 384]             768\n",
      "MultiheadAttention-279  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-280              [128, 1, 384]               0\n",
      "       LayerNorm-281              [128, 1, 384]             768\n",
      "          Linear-282             [128, 1, 1536]         591,360\n",
      "         Dropout-283             [128, 1, 1536]               0\n",
      "          Linear-284              [128, 1, 384]         590,208\n",
      "         Dropout-285              [128, 1, 384]               0\n",
      "    DecoderLayer-286  [[-1, 1, 384], [-1, 3, 384]]               0\n",
      "       LayerNorm-287              [128, 1, 384]             768\n",
      "         Decoder-288              [128, 1, 384]               0\n",
      "          Linear-289               [128, 1, 95]          36,575\n",
      "       Embedding-290              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-291              [128, 1, 384]               0\n",
      "       Embedding-292              [128, 3, 384]         902,400\n",
      "  TokenEmbedding-293              [128, 3, 384]               0\n",
      "         Dropout-294              [128, 4, 384]               0\n",
      "         Dropout-295              [128, 1, 384]               0\n",
      "       LayerNorm-296              [128, 1, 384]             768\n",
      "       LayerNorm-297              [128, 4, 384]             768\n",
      "MultiheadAttention-298  [[-1, 1, 384], [-1, 1, 4]]               0\n",
      "         Dropout-299              [128, 1, 384]               0\n",
      "       LayerNorm-300              [128, 1, 384]             768\n",
      "MultiheadAttention-301  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-302              [128, 1, 384]               0\n",
      "       LayerNorm-303              [128, 1, 384]             768\n",
      "          Linear-304             [128, 1, 1536]         591,360\n",
      "         Dropout-305             [128, 1, 1536]               0\n",
      "          Linear-306              [128, 1, 384]         590,208\n",
      "         Dropout-307              [128, 1, 384]               0\n",
      "    DecoderLayer-308  [[-1, 1, 384], [-1, 4, 384]]               0\n",
      "       LayerNorm-309              [128, 1, 384]             768\n",
      "         Decoder-310              [128, 1, 384]               0\n",
      "          Linear-311               [128, 1, 95]          36,575\n",
      "       Embedding-312              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-313              [128, 1, 384]               0\n",
      "       Embedding-314              [128, 4, 384]         902,400\n",
      "  TokenEmbedding-315              [128, 4, 384]               0\n",
      "         Dropout-316              [128, 5, 384]               0\n",
      "         Dropout-317              [128, 1, 384]               0\n",
      "       LayerNorm-318              [128, 1, 384]             768\n",
      "       LayerNorm-319              [128, 5, 384]             768\n",
      "MultiheadAttention-320  [[-1, 1, 384], [-1, 1, 5]]               0\n",
      "         Dropout-321              [128, 1, 384]               0\n",
      "       LayerNorm-322              [128, 1, 384]             768\n",
      "MultiheadAttention-323  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-324              [128, 1, 384]               0\n",
      "       LayerNorm-325              [128, 1, 384]             768\n",
      "          Linear-326             [128, 1, 1536]         591,360\n",
      "         Dropout-327             [128, 1, 1536]               0\n",
      "          Linear-328              [128, 1, 384]         590,208\n",
      "         Dropout-329              [128, 1, 384]               0\n",
      "    DecoderLayer-330  [[-1, 1, 384], [-1, 5, 384]]               0\n",
      "       LayerNorm-331              [128, 1, 384]             768\n",
      "         Decoder-332              [128, 1, 384]               0\n",
      "          Linear-333               [128, 1, 95]          36,575\n",
      "       Embedding-334              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-335              [128, 1, 384]               0\n",
      "       Embedding-336              [128, 5, 384]         902,400\n",
      "  TokenEmbedding-337              [128, 5, 384]               0\n",
      "         Dropout-338              [128, 6, 384]               0\n",
      "         Dropout-339              [128, 1, 384]               0\n",
      "       LayerNorm-340              [128, 1, 384]             768\n",
      "       LayerNorm-341              [128, 6, 384]             768\n",
      "MultiheadAttention-342  [[-1, 1, 384], [-1, 1, 6]]               0\n",
      "         Dropout-343              [128, 1, 384]               0\n",
      "       LayerNorm-344              [128, 1, 384]             768\n",
      "MultiheadAttention-345  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-346              [128, 1, 384]               0\n",
      "       LayerNorm-347              [128, 1, 384]             768\n",
      "          Linear-348             [128, 1, 1536]         591,360\n",
      "         Dropout-349             [128, 1, 1536]               0\n",
      "          Linear-350              [128, 1, 384]         590,208\n",
      "         Dropout-351              [128, 1, 384]               0\n",
      "    DecoderLayer-352  [[-1, 1, 384], [-1, 6, 384]]               0\n",
      "       LayerNorm-353              [128, 1, 384]             768\n",
      "         Decoder-354              [128, 1, 384]               0\n",
      "          Linear-355               [128, 1, 95]          36,575\n",
      "       Embedding-356              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-357              [128, 1, 384]               0\n",
      "       Embedding-358              [128, 6, 384]         902,400\n",
      "  TokenEmbedding-359              [128, 6, 384]               0\n",
      "         Dropout-360              [128, 7, 384]               0\n",
      "         Dropout-361              [128, 1, 384]               0\n",
      "       LayerNorm-362              [128, 1, 384]             768\n",
      "       LayerNorm-363              [128, 7, 384]             768\n",
      "MultiheadAttention-364  [[-1, 1, 384], [-1, 1, 7]]               0\n",
      "         Dropout-365              [128, 1, 384]               0\n",
      "       LayerNorm-366              [128, 1, 384]             768\n",
      "MultiheadAttention-367  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-368              [128, 1, 384]               0\n",
      "       LayerNorm-369              [128, 1, 384]             768\n",
      "          Linear-370             [128, 1, 1536]         591,360\n",
      "         Dropout-371             [128, 1, 1536]               0\n",
      "          Linear-372              [128, 1, 384]         590,208\n",
      "         Dropout-373              [128, 1, 384]               0\n",
      "    DecoderLayer-374  [[-1, 1, 384], [-1, 7, 384]]               0\n",
      "       LayerNorm-375              [128, 1, 384]             768\n",
      "         Decoder-376              [128, 1, 384]               0\n",
      "          Linear-377               [128, 1, 95]          36,575\n",
      "       Embedding-378              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-379              [128, 1, 384]               0\n",
      "       Embedding-380              [128, 7, 384]         902,400\n",
      "  TokenEmbedding-381              [128, 7, 384]               0\n",
      "         Dropout-382              [128, 8, 384]               0\n",
      "         Dropout-383              [128, 1, 384]               0\n",
      "       LayerNorm-384              [128, 1, 384]             768\n",
      "       LayerNorm-385              [128, 8, 384]             768\n",
      "MultiheadAttention-386  [[-1, 1, 384], [-1, 1, 8]]               0\n",
      "         Dropout-387              [128, 1, 384]               0\n",
      "       LayerNorm-388              [128, 1, 384]             768\n",
      "MultiheadAttention-389  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-390              [128, 1, 384]               0\n",
      "       LayerNorm-391              [128, 1, 384]             768\n",
      "          Linear-392             [128, 1, 1536]         591,360\n",
      "         Dropout-393             [128, 1, 1536]               0\n",
      "          Linear-394              [128, 1, 384]         590,208\n",
      "         Dropout-395              [128, 1, 384]               0\n",
      "    DecoderLayer-396  [[-1, 1, 384], [-1, 8, 384]]               0\n",
      "       LayerNorm-397              [128, 1, 384]             768\n",
      "         Decoder-398              [128, 1, 384]               0\n",
      "          Linear-399               [128, 1, 95]          36,575\n",
      "       Embedding-400              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-401              [128, 1, 384]               0\n",
      "       Embedding-402              [128, 8, 384]         902,400\n",
      "  TokenEmbedding-403              [128, 8, 384]               0\n",
      "         Dropout-404              [128, 9, 384]               0\n",
      "         Dropout-405              [128, 1, 384]               0\n",
      "       LayerNorm-406              [128, 1, 384]             768\n",
      "       LayerNorm-407              [128, 9, 384]             768\n",
      "MultiheadAttention-408  [[-1, 1, 384], [-1, 1, 9]]               0\n",
      "         Dropout-409              [128, 1, 384]               0\n",
      "       LayerNorm-410              [128, 1, 384]             768\n",
      "MultiheadAttention-411  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-412              [128, 1, 384]               0\n",
      "       LayerNorm-413              [128, 1, 384]             768\n",
      "          Linear-414             [128, 1, 1536]         591,360\n",
      "         Dropout-415             [128, 1, 1536]               0\n",
      "          Linear-416              [128, 1, 384]         590,208\n",
      "         Dropout-417              [128, 1, 384]               0\n",
      "    DecoderLayer-418  [[-1, 1, 384], [-1, 9, 384]]               0\n",
      "       LayerNorm-419              [128, 1, 384]             768\n",
      "         Decoder-420              [128, 1, 384]               0\n",
      "          Linear-421               [128, 1, 95]          36,575\n",
      "       Embedding-422              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-423              [128, 1, 384]               0\n",
      "       Embedding-424              [128, 9, 384]         902,400\n",
      "  TokenEmbedding-425              [128, 9, 384]               0\n",
      "         Dropout-426             [128, 10, 384]               0\n",
      "         Dropout-427              [128, 1, 384]               0\n",
      "       LayerNorm-428              [128, 1, 384]             768\n",
      "       LayerNorm-429             [128, 10, 384]             768\n",
      "MultiheadAttention-430  [[-1, 1, 384], [-1, 1, 10]]               0\n",
      "         Dropout-431              [128, 1, 384]               0\n",
      "       LayerNorm-432              [128, 1, 384]             768\n",
      "MultiheadAttention-433  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-434              [128, 1, 384]               0\n",
      "       LayerNorm-435              [128, 1, 384]             768\n",
      "          Linear-436             [128, 1, 1536]         591,360\n",
      "         Dropout-437             [128, 1, 1536]               0\n",
      "          Linear-438              [128, 1, 384]         590,208\n",
      "         Dropout-439              [128, 1, 384]               0\n",
      "    DecoderLayer-440  [[-1, 1, 384], [-1, 10, 384]]               0\n",
      "       LayerNorm-441              [128, 1, 384]             768\n",
      "         Decoder-442              [128, 1, 384]               0\n",
      "          Linear-443               [128, 1, 95]          36,575\n",
      "       Embedding-444              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-445              [128, 1, 384]               0\n",
      "       Embedding-446             [128, 10, 384]         902,400\n",
      "  TokenEmbedding-447             [128, 10, 384]               0\n",
      "         Dropout-448             [128, 11, 384]               0\n",
      "         Dropout-449              [128, 1, 384]               0\n",
      "       LayerNorm-450              [128, 1, 384]             768\n",
      "       LayerNorm-451             [128, 11, 384]             768\n",
      "MultiheadAttention-452  [[-1, 1, 384], [-1, 1, 11]]               0\n",
      "         Dropout-453              [128, 1, 384]               0\n",
      "       LayerNorm-454              [128, 1, 384]             768\n",
      "MultiheadAttention-455  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-456              [128, 1, 384]               0\n",
      "       LayerNorm-457              [128, 1, 384]             768\n",
      "          Linear-458             [128, 1, 1536]         591,360\n",
      "         Dropout-459             [128, 1, 1536]               0\n",
      "          Linear-460              [128, 1, 384]         590,208\n",
      "         Dropout-461              [128, 1, 384]               0\n",
      "    DecoderLayer-462  [[-1, 1, 384], [-1, 11, 384]]               0\n",
      "       LayerNorm-463              [128, 1, 384]             768\n",
      "         Decoder-464              [128, 1, 384]               0\n",
      "          Linear-465               [128, 1, 95]          36,575\n",
      "       Embedding-466              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-467              [128, 1, 384]               0\n",
      "       Embedding-468             [128, 11, 384]         902,400\n",
      "  TokenEmbedding-469             [128, 11, 384]               0\n",
      "         Dropout-470             [128, 12, 384]               0\n",
      "         Dropout-471              [128, 1, 384]               0\n",
      "       LayerNorm-472              [128, 1, 384]             768\n",
      "       LayerNorm-473             [128, 12, 384]             768\n",
      "MultiheadAttention-474  [[-1, 1, 384], [-1, 1, 12]]               0\n",
      "         Dropout-475              [128, 1, 384]               0\n",
      "       LayerNorm-476              [128, 1, 384]             768\n",
      "MultiheadAttention-477  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-478              [128, 1, 384]               0\n",
      "       LayerNorm-479              [128, 1, 384]             768\n",
      "          Linear-480             [128, 1, 1536]         591,360\n",
      "         Dropout-481             [128, 1, 1536]               0\n",
      "          Linear-482              [128, 1, 384]         590,208\n",
      "         Dropout-483              [128, 1, 384]               0\n",
      "    DecoderLayer-484  [[-1, 1, 384], [-1, 12, 384]]               0\n",
      "       LayerNorm-485              [128, 1, 384]             768\n",
      "         Decoder-486              [128, 1, 384]               0\n",
      "          Linear-487               [128, 1, 95]          36,575\n",
      "       Embedding-488              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-489              [128, 1, 384]               0\n",
      "       Embedding-490             [128, 12, 384]         902,400\n",
      "  TokenEmbedding-491             [128, 12, 384]               0\n",
      "         Dropout-492             [128, 13, 384]               0\n",
      "         Dropout-493              [128, 1, 384]               0\n",
      "       LayerNorm-494              [128, 1, 384]             768\n",
      "       LayerNorm-495             [128, 13, 384]             768\n",
      "MultiheadAttention-496  [[-1, 1, 384], [-1, 1, 13]]               0\n",
      "         Dropout-497              [128, 1, 384]               0\n",
      "       LayerNorm-498              [128, 1, 384]             768\n",
      "MultiheadAttention-499  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-500              [128, 1, 384]               0\n",
      "       LayerNorm-501              [128, 1, 384]             768\n",
      "          Linear-502             [128, 1, 1536]         591,360\n",
      "         Dropout-503             [128, 1, 1536]               0\n",
      "          Linear-504              [128, 1, 384]         590,208\n",
      "         Dropout-505              [128, 1, 384]               0\n",
      "    DecoderLayer-506  [[-1, 1, 384], [-1, 13, 384]]               0\n",
      "       LayerNorm-507              [128, 1, 384]             768\n",
      "         Decoder-508              [128, 1, 384]               0\n",
      "          Linear-509               [128, 1, 95]          36,575\n",
      "       Embedding-510              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-511              [128, 1, 384]               0\n",
      "       Embedding-512             [128, 13, 384]         902,400\n",
      "  TokenEmbedding-513             [128, 13, 384]               0\n",
      "         Dropout-514             [128, 14, 384]               0\n",
      "         Dropout-515              [128, 1, 384]               0\n",
      "       LayerNorm-516              [128, 1, 384]             768\n",
      "       LayerNorm-517             [128, 14, 384]             768\n",
      "MultiheadAttention-518  [[-1, 1, 384], [-1, 1, 14]]               0\n",
      "         Dropout-519              [128, 1, 384]               0\n",
      "       LayerNorm-520              [128, 1, 384]             768\n",
      "MultiheadAttention-521  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-522              [128, 1, 384]               0\n",
      "       LayerNorm-523              [128, 1, 384]             768\n",
      "          Linear-524             [128, 1, 1536]         591,360\n",
      "         Dropout-525             [128, 1, 1536]               0\n",
      "          Linear-526              [128, 1, 384]         590,208\n",
      "         Dropout-527              [128, 1, 384]               0\n",
      "    DecoderLayer-528  [[-1, 1, 384], [-1, 14, 384]]               0\n",
      "       LayerNorm-529              [128, 1, 384]             768\n",
      "         Decoder-530              [128, 1, 384]               0\n",
      "          Linear-531               [128, 1, 95]          36,575\n",
      "       Embedding-532              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-533              [128, 1, 384]               0\n",
      "       Embedding-534             [128, 14, 384]         902,400\n",
      "  TokenEmbedding-535             [128, 14, 384]               0\n",
      "         Dropout-536             [128, 15, 384]               0\n",
      "         Dropout-537              [128, 1, 384]               0\n",
      "       LayerNorm-538              [128, 1, 384]             768\n",
      "       LayerNorm-539             [128, 15, 384]             768\n",
      "MultiheadAttention-540  [[-1, 1, 384], [-1, 1, 15]]               0\n",
      "         Dropout-541              [128, 1, 384]               0\n",
      "       LayerNorm-542              [128, 1, 384]             768\n",
      "MultiheadAttention-543  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-544              [128, 1, 384]               0\n",
      "       LayerNorm-545              [128, 1, 384]             768\n",
      "          Linear-546             [128, 1, 1536]         591,360\n",
      "         Dropout-547             [128, 1, 1536]               0\n",
      "          Linear-548              [128, 1, 384]         590,208\n",
      "         Dropout-549              [128, 1, 384]               0\n",
      "    DecoderLayer-550  [[-1, 1, 384], [-1, 15, 384]]               0\n",
      "       LayerNorm-551              [128, 1, 384]             768\n",
      "         Decoder-552              [128, 1, 384]               0\n",
      "          Linear-553               [128, 1, 95]          36,575\n",
      "       Embedding-554              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-555              [128, 1, 384]               0\n",
      "       Embedding-556             [128, 15, 384]         902,400\n",
      "  TokenEmbedding-557             [128, 15, 384]               0\n",
      "         Dropout-558             [128, 16, 384]               0\n",
      "         Dropout-559              [128, 1, 384]               0\n",
      "       LayerNorm-560              [128, 1, 384]             768\n",
      "       LayerNorm-561             [128, 16, 384]             768\n",
      "MultiheadAttention-562  [[-1, 1, 384], [-1, 1, 16]]               0\n",
      "         Dropout-563              [128, 1, 384]               0\n",
      "       LayerNorm-564              [128, 1, 384]             768\n",
      "MultiheadAttention-565  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-566              [128, 1, 384]               0\n",
      "       LayerNorm-567              [128, 1, 384]             768\n",
      "          Linear-568             [128, 1, 1536]         591,360\n",
      "         Dropout-569             [128, 1, 1536]               0\n",
      "          Linear-570              [128, 1, 384]         590,208\n",
      "         Dropout-571              [128, 1, 384]               0\n",
      "    DecoderLayer-572  [[-1, 1, 384], [-1, 16, 384]]               0\n",
      "       LayerNorm-573              [128, 1, 384]             768\n",
      "         Decoder-574              [128, 1, 384]               0\n",
      "          Linear-575               [128, 1, 95]          36,575\n",
      "       Embedding-576              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-577              [128, 1, 384]               0\n",
      "       Embedding-578             [128, 16, 384]         902,400\n",
      "  TokenEmbedding-579             [128, 16, 384]               0\n",
      "         Dropout-580             [128, 17, 384]               0\n",
      "         Dropout-581              [128, 1, 384]               0\n",
      "       LayerNorm-582              [128, 1, 384]             768\n",
      "       LayerNorm-583             [128, 17, 384]             768\n",
      "MultiheadAttention-584  [[-1, 1, 384], [-1, 1, 17]]               0\n",
      "         Dropout-585              [128, 1, 384]               0\n",
      "       LayerNorm-586              [128, 1, 384]             768\n",
      "MultiheadAttention-587  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-588              [128, 1, 384]               0\n",
      "       LayerNorm-589              [128, 1, 384]             768\n",
      "          Linear-590             [128, 1, 1536]         591,360\n",
      "         Dropout-591             [128, 1, 1536]               0\n",
      "          Linear-592              [128, 1, 384]         590,208\n",
      "         Dropout-593              [128, 1, 384]               0\n",
      "    DecoderLayer-594  [[-1, 1, 384], [-1, 17, 384]]               0\n",
      "       LayerNorm-595              [128, 1, 384]             768\n",
      "         Decoder-596              [128, 1, 384]               0\n",
      "          Linear-597               [128, 1, 95]          36,575\n",
      "       Embedding-598              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-599              [128, 1, 384]               0\n",
      "       Embedding-600             [128, 17, 384]         902,400\n",
      "  TokenEmbedding-601             [128, 17, 384]               0\n",
      "         Dropout-602             [128, 18, 384]               0\n",
      "         Dropout-603              [128, 1, 384]               0\n",
      "       LayerNorm-604              [128, 1, 384]             768\n",
      "       LayerNorm-605             [128, 18, 384]             768\n",
      "MultiheadAttention-606  [[-1, 1, 384], [-1, 1, 18]]               0\n",
      "         Dropout-607              [128, 1, 384]               0\n",
      "       LayerNorm-608              [128, 1, 384]             768\n",
      "MultiheadAttention-609  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-610              [128, 1, 384]               0\n",
      "       LayerNorm-611              [128, 1, 384]             768\n",
      "          Linear-612             [128, 1, 1536]         591,360\n",
      "         Dropout-613             [128, 1, 1536]               0\n",
      "          Linear-614              [128, 1, 384]         590,208\n",
      "         Dropout-615              [128, 1, 384]               0\n",
      "    DecoderLayer-616  [[-1, 1, 384], [-1, 18, 384]]               0\n",
      "       LayerNorm-617              [128, 1, 384]             768\n",
      "         Decoder-618              [128, 1, 384]               0\n",
      "          Linear-619               [128, 1, 95]          36,575\n",
      "       Embedding-620              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-621              [128, 1, 384]               0\n",
      "       Embedding-622             [128, 18, 384]         902,400\n",
      "  TokenEmbedding-623             [128, 18, 384]               0\n",
      "         Dropout-624             [128, 19, 384]               0\n",
      "         Dropout-625              [128, 1, 384]               0\n",
      "       LayerNorm-626              [128, 1, 384]             768\n",
      "       LayerNorm-627             [128, 19, 384]             768\n",
      "MultiheadAttention-628  [[-1, 1, 384], [-1, 1, 19]]               0\n",
      "         Dropout-629              [128, 1, 384]               0\n",
      "       LayerNorm-630              [128, 1, 384]             768\n",
      "MultiheadAttention-631  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-632              [128, 1, 384]               0\n",
      "       LayerNorm-633              [128, 1, 384]             768\n",
      "          Linear-634             [128, 1, 1536]         591,360\n",
      "         Dropout-635             [128, 1, 1536]               0\n",
      "          Linear-636              [128, 1, 384]         590,208\n",
      "         Dropout-637              [128, 1, 384]               0\n",
      "    DecoderLayer-638  [[-1, 1, 384], [-1, 19, 384]]               0\n",
      "       LayerNorm-639              [128, 1, 384]             768\n",
      "         Decoder-640              [128, 1, 384]               0\n",
      "          Linear-641               [128, 1, 95]          36,575\n",
      "       Embedding-642              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-643              [128, 1, 384]               0\n",
      "       Embedding-644             [128, 19, 384]         902,400\n",
      "  TokenEmbedding-645             [128, 19, 384]               0\n",
      "         Dropout-646             [128, 20, 384]               0\n",
      "         Dropout-647              [128, 1, 384]               0\n",
      "       LayerNorm-648              [128, 1, 384]             768\n",
      "       LayerNorm-649             [128, 20, 384]             768\n",
      "MultiheadAttention-650  [[-1, 1, 384], [-1, 1, 20]]               0\n",
      "         Dropout-651              [128, 1, 384]               0\n",
      "       LayerNorm-652              [128, 1, 384]             768\n",
      "MultiheadAttention-653  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-654              [128, 1, 384]               0\n",
      "       LayerNorm-655              [128, 1, 384]             768\n",
      "          Linear-656             [128, 1, 1536]         591,360\n",
      "         Dropout-657             [128, 1, 1536]               0\n",
      "          Linear-658              [128, 1, 384]         590,208\n",
      "         Dropout-659              [128, 1, 384]               0\n",
      "    DecoderLayer-660  [[-1, 1, 384], [-1, 20, 384]]               0\n",
      "       LayerNorm-661              [128, 1, 384]             768\n",
      "         Decoder-662              [128, 1, 384]               0\n",
      "          Linear-663               [128, 1, 95]          36,575\n",
      "       Embedding-664              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-665              [128, 1, 384]               0\n",
      "       Embedding-666             [128, 20, 384]         902,400\n",
      "  TokenEmbedding-667             [128, 20, 384]               0\n",
      "         Dropout-668             [128, 21, 384]               0\n",
      "         Dropout-669              [128, 1, 384]               0\n",
      "       LayerNorm-670              [128, 1, 384]             768\n",
      "       LayerNorm-671             [128, 21, 384]             768\n",
      "MultiheadAttention-672  [[-1, 1, 384], [-1, 1, 21]]               0\n",
      "         Dropout-673              [128, 1, 384]               0\n",
      "       LayerNorm-674              [128, 1, 384]             768\n",
      "MultiheadAttention-675  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-676              [128, 1, 384]               0\n",
      "       LayerNorm-677              [128, 1, 384]             768\n",
      "          Linear-678             [128, 1, 1536]         591,360\n",
      "         Dropout-679             [128, 1, 1536]               0\n",
      "          Linear-680              [128, 1, 384]         590,208\n",
      "         Dropout-681              [128, 1, 384]               0\n",
      "    DecoderLayer-682  [[-1, 1, 384], [-1, 21, 384]]               0\n",
      "       LayerNorm-683              [128, 1, 384]             768\n",
      "         Decoder-684              [128, 1, 384]               0\n",
      "          Linear-685               [128, 1, 95]          36,575\n",
      "       Embedding-686              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-687              [128, 1, 384]               0\n",
      "       Embedding-688             [128, 21, 384]         902,400\n",
      "  TokenEmbedding-689             [128, 21, 384]               0\n",
      "         Dropout-690             [128, 22, 384]               0\n",
      "         Dropout-691              [128, 1, 384]               0\n",
      "       LayerNorm-692              [128, 1, 384]             768\n",
      "       LayerNorm-693             [128, 22, 384]             768\n",
      "MultiheadAttention-694  [[-1, 1, 384], [-1, 1, 22]]               0\n",
      "         Dropout-695              [128, 1, 384]               0\n",
      "       LayerNorm-696              [128, 1, 384]             768\n",
      "MultiheadAttention-697  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-698              [128, 1, 384]               0\n",
      "       LayerNorm-699              [128, 1, 384]             768\n",
      "          Linear-700             [128, 1, 1536]         591,360\n",
      "         Dropout-701             [128, 1, 1536]               0\n",
      "          Linear-702              [128, 1, 384]         590,208\n",
      "         Dropout-703              [128, 1, 384]               0\n",
      "    DecoderLayer-704  [[-1, 1, 384], [-1, 22, 384]]               0\n",
      "       LayerNorm-705              [128, 1, 384]             768\n",
      "         Decoder-706              [128, 1, 384]               0\n",
      "          Linear-707               [128, 1, 95]          36,575\n",
      "       Embedding-708              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-709              [128, 1, 384]               0\n",
      "       Embedding-710             [128, 22, 384]         902,400\n",
      "  TokenEmbedding-711             [128, 22, 384]               0\n",
      "         Dropout-712             [128, 23, 384]               0\n",
      "         Dropout-713              [128, 1, 384]               0\n",
      "       LayerNorm-714              [128, 1, 384]             768\n",
      "       LayerNorm-715             [128, 23, 384]             768\n",
      "MultiheadAttention-716  [[-1, 1, 384], [-1, 1, 23]]               0\n",
      "         Dropout-717              [128, 1, 384]               0\n",
      "       LayerNorm-718              [128, 1, 384]             768\n",
      "MultiheadAttention-719  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-720              [128, 1, 384]               0\n",
      "       LayerNorm-721              [128, 1, 384]             768\n",
      "          Linear-722             [128, 1, 1536]         591,360\n",
      "         Dropout-723             [128, 1, 1536]               0\n",
      "          Linear-724              [128, 1, 384]         590,208\n",
      "         Dropout-725              [128, 1, 384]               0\n",
      "    DecoderLayer-726  [[-1, 1, 384], [-1, 23, 384]]               0\n",
      "       LayerNorm-727              [128, 1, 384]             768\n",
      "         Decoder-728              [128, 1, 384]               0\n",
      "          Linear-729               [128, 1, 95]          36,575\n",
      "       Embedding-730              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-731              [128, 1, 384]               0\n",
      "       Embedding-732             [128, 23, 384]         902,400\n",
      "  TokenEmbedding-733             [128, 23, 384]               0\n",
      "         Dropout-734             [128, 24, 384]               0\n",
      "         Dropout-735              [128, 1, 384]               0\n",
      "       LayerNorm-736              [128, 1, 384]             768\n",
      "       LayerNorm-737             [128, 24, 384]             768\n",
      "MultiheadAttention-738  [[-1, 1, 384], [-1, 1, 24]]               0\n",
      "         Dropout-739              [128, 1, 384]               0\n",
      "       LayerNorm-740              [128, 1, 384]             768\n",
      "MultiheadAttention-741  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-742              [128, 1, 384]               0\n",
      "       LayerNorm-743              [128, 1, 384]             768\n",
      "          Linear-744             [128, 1, 1536]         591,360\n",
      "         Dropout-745             [128, 1, 1536]               0\n",
      "          Linear-746              [128, 1, 384]         590,208\n",
      "         Dropout-747              [128, 1, 384]               0\n",
      "    DecoderLayer-748  [[-1, 1, 384], [-1, 24, 384]]               0\n",
      "       LayerNorm-749              [128, 1, 384]             768\n",
      "         Decoder-750              [128, 1, 384]               0\n",
      "          Linear-751               [128, 1, 95]          36,575\n",
      "       Embedding-752              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-753              [128, 1, 384]               0\n",
      "       Embedding-754             [128, 24, 384]         902,400\n",
      "  TokenEmbedding-755             [128, 24, 384]               0\n",
      "         Dropout-756             [128, 25, 384]               0\n",
      "         Dropout-757              [128, 1, 384]               0\n",
      "       LayerNorm-758              [128, 1, 384]             768\n",
      "       LayerNorm-759             [128, 25, 384]             768\n",
      "MultiheadAttention-760  [[-1, 1, 384], [-1, 1, 25]]               0\n",
      "         Dropout-761              [128, 1, 384]               0\n",
      "       LayerNorm-762              [128, 1, 384]             768\n",
      "MultiheadAttention-763  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-764              [128, 1, 384]               0\n",
      "       LayerNorm-765              [128, 1, 384]             768\n",
      "          Linear-766             [128, 1, 1536]         591,360\n",
      "         Dropout-767             [128, 1, 1536]               0\n",
      "          Linear-768              [128, 1, 384]         590,208\n",
      "         Dropout-769              [128, 1, 384]               0\n",
      "    DecoderLayer-770  [[-1, 1, 384], [-1, 25, 384]]               0\n",
      "       LayerNorm-771              [128, 1, 384]             768\n",
      "         Decoder-772              [128, 1, 384]               0\n",
      "          Linear-773               [128, 1, 95]          36,575\n",
      "       Embedding-774              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-775              [128, 1, 384]               0\n",
      "       Embedding-776             [128, 25, 384]         902,400\n",
      "  TokenEmbedding-777             [128, 25, 384]               0\n",
      "         Dropout-778             [128, 26, 384]               0\n",
      "         Dropout-779              [128, 1, 384]               0\n",
      "       LayerNorm-780              [128, 1, 384]             768\n",
      "       LayerNorm-781             [128, 26, 384]             768\n",
      "MultiheadAttention-782  [[-1, 1, 384], [-1, 1, 26]]               0\n",
      "         Dropout-783              [128, 1, 384]               0\n",
      "       LayerNorm-784              [128, 1, 384]             768\n",
      "MultiheadAttention-785  [[-1, 1, 384], [-1, 1, 128]]               0\n",
      "         Dropout-786              [128, 1, 384]               0\n",
      "       LayerNorm-787              [128, 1, 384]             768\n",
      "          Linear-788             [128, 1, 1536]         591,360\n",
      "         Dropout-789             [128, 1, 1536]               0\n",
      "          Linear-790              [128, 1, 384]         590,208\n",
      "         Dropout-791              [128, 1, 384]               0\n",
      "    DecoderLayer-792  [[-1, 1, 384], [-1, 26, 384]]               0\n",
      "       LayerNorm-793              [128, 1, 384]             768\n",
      "         Decoder-794              [128, 1, 384]               0\n",
      "          Linear-795               [128, 1, 95]          36,575\n",
      "       Embedding-796              [128, 1, 384]         902,400\n",
      "  TokenEmbedding-797              [128, 1, 384]               0\n",
      "       Embedding-798             [128, 25, 384]         902,400\n",
      "  TokenEmbedding-799             [128, 25, 384]               0\n",
      "         Dropout-800             [128, 26, 384]               0\n",
      "         Dropout-801             [128, 26, 384]               0\n",
      "       LayerNorm-802             [128, 26, 384]             768\n",
      "       LayerNorm-803             [128, 26, 384]             768\n",
      "MultiheadAttention-804  [[-1, 26, 384], [-1, 26, 26]]               0\n",
      "         Dropout-805             [128, 26, 384]               0\n",
      "       LayerNorm-806             [128, 26, 384]             768\n",
      "MultiheadAttention-807  [[-1, 26, 384], [-1, 26, 128]]               0\n",
      "         Dropout-808             [128, 26, 384]               0\n",
      "       LayerNorm-809             [128, 26, 384]             768\n",
      "          Linear-810            [128, 26, 1536]         591,360\n",
      "         Dropout-811            [128, 26, 1536]               0\n",
      "          Linear-812             [128, 26, 384]         590,208\n",
      "         Dropout-813             [128, 26, 384]               0\n",
      "    DecoderLayer-814  [[-1, 26, 384], [-1, 26, 384]]               0\n",
      "       LayerNorm-815             [128, 26, 384]             768\n",
      "         Decoder-816             [128, 26, 384]               0\n",
      "          Linear-817              [128, 26, 95]          36,575\n",
      "================================================================\n",
      "Total params: 103,054,725\n",
      "Trainable params: 103,054,725\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 6.00\n",
      "Forward/backward pass size (MB): 12759.53\n",
      "Params size (MB): 393.12\n",
      "Estimated Total Size (MB): 13158.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, CFG['IMG_HEIGHT_SIZE'], CFG['IMG_WIDTH_SIZE']), batch_size=CFG['BATCH_SIZE'], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c04ab11-abe8-45b4-9ca2-f244087539bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be6ec84207843d29f1d573d4e3ce86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "target_lengths must be of size batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35776\\1476198850.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minfer_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35776\\3218359265.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mtext_batch_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_batch_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_35776\\229498674.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(text_batch, text_batch_logits)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtext_batch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_batch_targets_lens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_text_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_batch_logps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_batch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_batch_logps_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_batch_targets_lens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1743\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1744\u001b[1;33m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0m\u001b[0;32m   1745\u001b[0m                           self.zero_infinity)\n\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[1;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[0;32m   2614\u001b[0m             \u001b[0mblank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2615\u001b[0m         )\n\u001b[1;32m-> 2616\u001b[1;33m     return torch.ctc_loss(\n\u001b[0m\u001b[0;32m   2617\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2618\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: target_lengths must be of size batch_size"
     ]
    }
   ],
   "source": [
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "298cb190-ce56-4e72-80f8-e33c1858da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(f\"./result/{dir_name}/model.pt\", map_location= device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b807d-ac45-4183-b92d-2b40f154e66c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23e20fdc-ce3e-49c6-899d-317dde3316c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5705b13-5246-4519-b11d-75102322c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['img_path'].values, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8dcea4c-d9b2-44d6-9f5d-89c7108145da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "image_batch = iter(test_loader).next()\n",
    "print(image_batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45ea28d1-9cf1-410a-8dd2-da5eca91315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3760a24-bad7-4cb4-927c-b6007d223947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 95])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "525a225f-3296-4489-8a2b-8d322ff3d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8a121d99a0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = image_batch[206]\n",
    "img = img.cpu().numpy().T\n",
    "img = np.swapaxes(img, 0, 1)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fa2bd065-4f58-4b45-927b-17759e5dc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch in tqdm(iter(test_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            \n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "            \n",
    "            preds.extend(text_batch_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57711ef8-048b-481d-8c98-759a9c81f2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff94c1a81be4574b6aa058dcb6768e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3904ed-61af-4db6-896c-2bcd21709b9f",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87e53da9-eb17-4e9a-a5d7-857143128961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 별 추론결과를 독립적으로 후처리\n",
    "def remove_duplicates(text):\n",
    "    if len(text) > 1:\n",
    "        letters = [text[0]] + [letter for idx, letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n",
    "    elif len(text) == 1:\n",
    "        letters = [text[0]]\n",
    "    else:\n",
    "        return \"\"\n",
    "    return \"\".join(letters)\n",
    "\n",
    "def correct_prediction(word):\n",
    "    parts = word.split(\"-\")\n",
    "    parts = [remove_duplicates(part) for part in parts]\n",
    "    corrected_word = \"\".join(parts)\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55c6f77c-afe4-4a75-848f-757e498f2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['label'] = predictions\n",
    "submit['label'] = submit['label'].apply(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d5f4863-49d8-4e90-92c8-b500e0b275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submission4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cab54e58-ac20-4726-8fa1-3aa763514a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-날-------말--------',\n",
       " '-상-------황--------',\n",
       " '-받-아---들---이--다---',\n",
       " '-바------구----늬----',\n",
       " '-살----------------',\n",
       " '-빼----놓-----다-----',\n",
       " '-인---식---하---다----',\n",
       " '-센------터---------',\n",
       " '-소------풍---------',\n",
       " '-광--------주-------',\n",
       " '-나-------낙--------',\n",
       " '-위-------험--------',\n",
       " '-도-------도--------',\n",
       " '-술------표---------',\n",
       " '-괴-로-워----하---다---',\n",
       " '-카-------드--------',\n",
       " '-합-----치치--다다-----',\n",
       " '-다---정--하하--다다----',\n",
       " '-톤-------자자-------',\n",
       " '-가--능----하----다---',\n",
       " '-호-------주--------',\n",
       " '-발---전---되---다----',\n",
       " '-피----우----다------',\n",
       " '-스----웨-----터-----',\n",
       " '-시---작---되---다----',\n",
       " '-겨-------울--------',\n",
       " '-예--------선-------',\n",
       " '-한----국-----말-----',\n",
       " '-세---워---지---다----',\n",
       " '-비----방----밥------',\n",
       " '-좋------다다--------',\n",
       " '-남-대---문---시---장--',\n",
       " '-보---수------적-----',\n",
       " '-사----진-----기-----',\n",
       " '-내---리-----다다-----',\n",
       " '-핑------핑---------',\n",
       " '-맛---없----다다------',\n",
       " '-특----별-----히-----',\n",
       " '-우-------선선-------',\n",
       " '-대-----답답---------',\n",
       " '-학--------생-------',\n",
       " '-여------덟---------',\n",
       " '-본-------질--------',\n",
       " '-현-------대--------',\n",
       " '-속---하------다-----',\n",
       " '-지--저--분---하--다---',\n",
       " '-불-------다--------',\n",
       " '-아-------래--------',\n",
       " '-걸--어----오----다---',\n",
       " '-선------원---------',\n",
       " '-호------주---------',\n",
       " '-시----------------',\n",
       " '-골----프-----장-----',\n",
       " '-가--------위-------',\n",
       " '-기-------차--------',\n",
       " '-저-------것--------',\n",
       " '-기------침---------',\n",
       " '-위--------쪽-------',\n",
       " '-불-----만----------',\n",
       " '-바-람---직--하---다---',\n",
       " '-작--아----지---다----',\n",
       " '-학-------비--------',\n",
       " '-양-------말--------',\n",
       " '-피-------곤--------',\n",
       " '-말-------씀--------',\n",
       " '-아-----무----튼-----',\n",
       " '-뱀----------------',\n",
       " '-이-------모--------',\n",
       " '-역----사-----상-----',\n",
       " '-공--개---하----다----',\n",
       " '-영----화-----관-----',\n",
       " '-피-------자--------',\n",
       " '-위----------------',\n",
       " '-세----------------',\n",
       " '-있-----다----------',\n",
       " '-사---------십------',\n",
       " '-선------물---------',\n",
       " '-아-------직--------',\n",
       " '-결--심----하----다---',\n",
       " '-블--라----우----스---',\n",
       " '-이-------자--------',\n",
       " '-트----리-----다-----',\n",
       " '-지----우-----개-----',\n",
       " '-소-------망--------',\n",
       " '-시----------------',\n",
       " '-울------산---------',\n",
       " '-끌-----다----------',\n",
       " '-바-------미--------',\n",
       " '-깨---끗--해--지--다---',\n",
       " '-소----규-----모-----',\n",
       " '-나--아----지---다다---',\n",
       " '-밀----리-----다-----',\n",
       " '-예----------------',\n",
       " '-분----------------',\n",
       " '-바-----나----다-----',\n",
       " '-경-------우--------',\n",
       " '-마----지지----막-----',\n",
       " '-시----------------',\n",
       " '-소------리---------',\n",
       " '-한------사---------',\n",
       " '-답-------다--------',\n",
       " '-왜----------------',\n",
       " '-올----래-----------',\n",
       " '-불-가--능---하--다----',\n",
       " '-체---험--하하---다----',\n",
       " '-땅----바-----닥-----',\n",
       " '-돌--아----오---다다---',\n",
       " '-아----래-----쪽-----',\n",
       " '-말--씀---하----다----',\n",
       " '-한----순-----간-----',\n",
       " '-도-------도--------',\n",
       " '-오------래---------',\n",
       " '-빨-------다--------',\n",
       " '-항-------공--------',\n",
       " '-생--각----하---다----',\n",
       " '-남-------북--------',\n",
       " '-반------반반--------',\n",
       " '-논-------문--------',\n",
       " '-누----르-----다-----',\n",
       " '-저-------축--------',\n",
       " '-둘--러----싸----다---',\n",
       " '-내--려----가---다----',\n",
       " '-공----식-----적-----',\n",
       " '-사-------과--------',\n",
       " '-허--용----하----다---',\n",
       " '-서-------울--------',\n",
       " '-아----무-----런-----',\n",
       " '-돌--보-----다다------',\n",
       " '-밑--바------닥------',\n",
       " '------------------',\n",
       " '-실-----은----------',\n",
       " '-흐--려---지---다-----',\n",
       " '-다--------섯-------',\n",
       " '-골-------프--------',\n",
       " '-처-------벌--------',\n",
       " '-한-----국----말-----',\n",
       " '-서------다---------',\n",
       " '-나------다---------',\n",
       " '-씨----------------',\n",
       " '-일----요----일------',\n",
       " '-환-------자--------',\n",
       " '-갈------다다--------',\n",
       " '-복--집집---하---다----',\n",
       " '-보------험---------',\n",
       " '-기----------------',\n",
       " '-분-------석--------',\n",
       " '---------월--------',\n",
       " '-관-------습--------',\n",
       " '-배----------------',\n",
       " '-반----기---다-------',\n",
       " '-천----------------',\n",
       " '-덜----------------',\n",
       " '-분--리---하----다----',\n",
       " '-미-------국--------',\n",
       " '-맺-------다--------',\n",
       " '-확---------일------',\n",
       " '-보-------통--------',\n",
       " '-저-------쪽--------',\n",
       " '-끝----내----다다-----',\n",
       " '-면--------적-------',\n",
       " '-다-------행--------',\n",
       " '-내--------일-------',\n",
       " '-짐-------질--------',\n",
       " '-사----실-----상-----',\n",
       " '-다-------리--------',\n",
       " '-편-------견--------',\n",
       " '-회------심---------',\n",
       " '-지----우-----다-----',\n",
       " '-날------다---------',\n",
       " '-여------디---------',\n",
       " '-저------쪽---------',\n",
       " '-휴-----지----통-----',\n",
       " '-얼----마-----나-----',\n",
       " '-단-------순--------',\n",
       " '-열-----다----------',\n",
       " '-많-------이--------',\n",
       " '-바-------퀴--------',\n",
       " '-반---------히------',\n",
       " '-생----기기---다------',\n",
       " '-씨----------------',\n",
       " '-가-------게--------',\n",
       " '------------------',\n",
       " '-개--발---하----다----',\n",
       " '-통------화---------',\n",
       " '-전-------체--------',\n",
       " '-시------계---------',\n",
       " '-수----돗-----물-----',\n",
       " '-불-------만--------',\n",
       " '-시----스----템------',\n",
       " '-깨----달-----음-----',\n",
       " '-다--짐----하---다----',\n",
       " '-가-----운---례------',\n",
       " '-되-돌--아---오---다---',\n",
       " '-풀--라----스----특---',\n",
       " '-설-어--뜨---리--다----',\n",
       " '-주------변---------',\n",
       " '-어--------제-------',\n",
       " '-번----------------',\n",
       " '-농----산-----물-----',\n",
       " '-병-------명--------',\n",
       " '-승--------진-------',\n",
       " '-치----------------',\n",
       " '-오------이---------',\n",
       " '-속------옷---------',\n",
       " '-요-------리--------',\n",
       " '-치-------마--------',\n",
       " '------------------',\n",
       " '-이--러---하----다----',\n",
       " '-신------문---------',\n",
       " '-희----런-----도-----',\n",
       " '------------------',\n",
       " '-피----------------',\n",
       " '-졸-------업--------',\n",
       " '-간----접-----적-----',\n",
       " '-세----시----다------',\n",
       " '-낳------다---------',\n",
       " '-발--생---하----다----',\n",
       " '-진-------통--------',\n",
       " '-왜----------------',\n",
       " '-사-------촌--------',\n",
       " '-너-------두--------',\n",
       " '-할----------------',\n",
       " '-더-----욱----이-----',\n",
       " '-출--발---하----다----',\n",
       " '-외------다다--------',\n",
       " '-않-------다--------',\n",
       " '-묶----이----다------',\n",
       " '-아----무-----것-----',\n",
       " '-저------축---------',\n",
       " '-채----------------',\n",
       " '-말---------기------',\n",
       " '-농------장---------',\n",
       " '-실------수---------',\n",
       " '-서------랍---------',\n",
       " '-파-----랗----다-----',\n",
       " '-상----상-----력-----',\n",
       " '-표-------시--------',\n",
       " '-팬-------티--------',\n",
       " '-어--물-----히--다----',\n",
       " '-말-------다--------',\n",
       " '-어-----제제---------',\n",
       " '-바----쁘-----다-----',\n",
       " '-진-------출--------',\n",
       " '-붉-----다----------',\n",
       " '-잠-------깐--------',\n",
       " '-정------지지--------',\n",
       " '-땅--------콩-------',\n",
       " '-솜-------씨--------',\n",
       " '-스--트----레----스---',\n",
       " '-거-------리--------',\n",
       " '-텅-------면--------',\n",
       " '-운-------문--------',\n",
       " '-무---지지-----개-----',\n",
       " '-앞-------문--------',\n",
       " '-모----니----터------',\n",
       " '-넷----------------',\n",
       " '-대-------답--------',\n",
       " '-까-------닭--------',\n",
       " '-비--비----다다-------',\n",
       " '-도---달----하---다---',\n",
       " '-빛---나----다다------',\n",
       " '-걸-----다----------',\n",
       " '-변------화---------',\n",
       " '-체-------력--------',\n",
       " '-제-----시----다-----',\n",
       " '-배-------비--------',\n",
       " '-명-------명--------',\n",
       " '-초-------기기-------',\n",
       " '-한-----국----어-----',\n",
       " '-방---행---하---다----',\n",
       " '-답-------장--------',\n",
       " '-지--도---하---다-----',\n",
       " '-최-----소---한------',\n",
       " '-시----회-----------',\n",
       " '-아--------들-------',\n",
       " '-유--능--하----다다----',\n",
       " '-아-------마--------',\n",
       " '-전--기---밥----솥----',\n",
       " '-수-------필--------',\n",
       " '-드----리-----다-----',\n",
       " '-현-----찮----다-----',\n",
       " '-걱--정---되---다-----',\n",
       " '-부-------디--------',\n",
       " '-마-------마--------',\n",
       " '-동--의----하----다---',\n",
       " '-한----평-----생-----',\n",
       " '-스--튜----디----오---',\n",
       " '-현----드----컵------',\n",
       " '-카------폐---------',\n",
       " '-속--삭----이----다---',\n",
       " '-중--요----하----다---',\n",
       " '-골---짜-------기----',\n",
       " '-피--------시-------',\n",
       " '-싸-------다다-------',\n",
       " '-지------하---------',\n",
       " '-홍----------------',\n",
       " '-미-------마--------',\n",
       " '-어------디---------',\n",
       " '-이-------분--------',\n",
       " '-갸--------계-------',\n",
       " '-일----본----어어-----',\n",
       " '-수----요-----일-----',\n",
       " '-에----어----컨------',\n",
       " '-첫-------째--------',\n",
       " '-하-----만-----색----',\n",
       " '-수-------십--------',\n",
       " '-내-------외--------',\n",
       " '-밥----------------',\n",
       " '-김--------치-------',\n",
       " '-옆-----구----리-----',\n",
       " '-네----------------',\n",
       " '-불------빛---------',\n",
       " '-소--개개---하---다----',\n",
       " '-얼---리----다다------',\n",
       " '-남----동-----생-----',\n",
       " '-사----무-----직-----',\n",
       " '-묻------다---------',\n",
       " '-비--------누-------',\n",
       " '-수--리----하---다----',\n",
       " '-소------문---------',\n",
       " '-세----------------',\n",
       " '-잃--어--비---하--다---',\n",
       " '-씨----------------',\n",
       " '-배----------------',\n",
       " '-머---리---카----락---',\n",
       " '-찬----------------',\n",
       " '-국--------적-------',\n",
       " '-재-------산--------',\n",
       " '-이-------것--------',\n",
       " '-증-------상--------',\n",
       " '-모------래---------',\n",
       " '-시----------------',\n",
       " '-활--------기-------',\n",
       " '-씨----------------',\n",
       " '-커-------다--------',\n",
       " '-사----------------',\n",
       " '-놓-아----두---다-----',\n",
       " '-빠-겨---나---가---다--',\n",
       " '-현------금---------',\n",
       " '-개--선----하----다---',\n",
       " '-뇌----------------',\n",
       " '-소--중----하----다---',\n",
       " '-노--래----하----다---',\n",
       " '-타-------락--------',\n",
       " '-학-------생생-------',\n",
       " '-쳐--다----보--다-----',\n",
       " '-밖----------------',\n",
       " '-가-------올--------',\n",
       " '-씨----사사----------',\n",
       " '-나-------라--------',\n",
       " '-학--------교-------',\n",
       " '-사----실-----상-----',\n",
       " '-외----아-----들-----',\n",
       " '-소-------문--------',\n",
       " '-위----------------',\n",
       " '-심-------심--------',\n",
       " '-낮-------다--------',\n",
       " '-꾸---준준----히------',\n",
       " '-다----섯-----째-----',\n",
       " '-안--심----하----다---',\n",
       " '-실-------수--------',\n",
       " '-사-------슴--------',\n",
       " '-한-------쪽--------',\n",
       " '-글---쓰------기-----',\n",
       " '-신------경---------',\n",
       " '-둘--러----보---다----',\n",
       " '-짧------다다--------',\n",
       " '-시------장---------',\n",
       " '-추-------측--------',\n",
       " '-불-확--실---하---다---',\n",
       " '-신---속---하---다----',\n",
       " '-활--동----하----다---',\n",
       " '-역----------------',\n",
       " '-사-----생---활------',\n",
       " '-연------애---------',\n",
       " '-음--------식-------',\n",
       " '-매--배----하---다----',\n",
       " '-튀----------------',\n",
       " '-파-----란-----색----',\n",
       " '-사-------실--------',\n",
       " '-그--------럼-------',\n",
       " '-양-------보--------',\n",
       " '-정------말---------',\n",
       " '-기-------억--------',\n",
       " '-개---선---되--다-----',\n",
       " '-소------다---------',\n",
       " '-예---약---하---다----',\n",
       " '-시-----부----모-----',\n",
       " '-광-------고--------',\n",
       " '-번--------개-------',\n",
       " '-행--복----하---다----',\n",
       " '-부--끄----럽----다---',\n",
       " '-서------다---------',\n",
       " '-도---시-----락------',\n",
       " '-조--------직-------',\n",
       " '-솔--직----하----다---',\n",
       " '-피----우---다-------',\n",
       " '-관-------심--------',\n",
       " '-요----리-----사-----',\n",
       " '-두----------------',\n",
       " '-고---생---하---다----',\n",
       " '-암--------시-------',\n",
       " '-잊-----다----------',\n",
       " '-지--원----하---다----',\n",
       " '-참--여----하----다---',\n",
       " '-머--리---카카---락----',\n",
       " '-되-------닥--------',\n",
       " '-학-----생-----증----',\n",
       " '-무-------렵--------',\n",
       " '-삼-----월----------',\n",
       " '-항------공---기-----',\n",
       " '-찌-------개개-------',\n",
       " '-평--가----되----다---',\n",
       " '-영-------양--------',\n",
       " '-풍----꾸-----다-----',\n",
       " '-두------엇---------',\n",
       " '-거------율---------',\n",
       " '-기--념----하----다---',\n",
       " '-이--룩----지----다---',\n",
       " '-복----화------경----',\n",
       " '-정----신---과-------',\n",
       " '-허--락----하----다---',\n",
       " '-춤----추-----다-----',\n",
       " '-제--------시-------',\n",
       " '-약-------속--------',\n",
       " '-마---련--하----다----',\n",
       " '-청----소-----년-----',\n",
       " '-경--------치-------',\n",
       " '-본----------------',\n",
       " '-아-------무--------',\n",
       " '-첫-------째--------',\n",
       " '-한------골---------',\n",
       " '-만-------약--------',\n",
       " '-가----깝-----다-----',\n",
       " '-지-원----하----다----',\n",
       " '-군------대---------',\n",
       " '-오---르------다-----',\n",
       " '-삼-------월--------',\n",
       " '-시----------------',\n",
       " '-도----시-----락-----',\n",
       " '-부--서---지지--다-----',\n",
       " '-병------원---------',\n",
       " '-아-----시----다-----',\n",
       " '-씹-----다----------',\n",
       " '-백--------하---다---',\n",
       " '-익--사---하----다----',\n",
       " '-연-------장--------',\n",
       " '-향----하------다----',\n",
       " '-흑-------백--------',\n",
       " '-가----하-----다-----',\n",
       " '-버-------룻--------',\n",
       " '-밝---혀---내---다----',\n",
       " '-물---곡---이----이---',\n",
       " '-김--포포--공----항----',\n",
       " '-보---호---하---다----',\n",
       " '-접-------돈--------',\n",
       " '-미-끄---러---지-다다---',\n",
       " '-세----------------',\n",
       " '-쇠-------파--------',\n",
       " '-남----기----다------',\n",
       " '-뛰-------다--------',\n",
       " '-상--------상-------',\n",
       " '-진-------리--------',\n",
       " '-맞-------문--------',\n",
       " '-세-------수--------',\n",
       " '-감---상---하---다----',\n",
       " '-싫-----다다---------',\n",
       " '-목----하----다------',\n",
       " '-우-------려--------',\n",
       " '-걱-------율--------',\n",
       " '-보---내-----다------',\n",
       " '-생-------활--------',\n",
       " '-뿌----리----다------',\n",
       " '-목-------욕--------',\n",
       " '-해----------------',\n",
       " '-데----이-----트-----',\n",
       " '-겁-------지--------',\n",
       " '-식------기---------',\n",
       " '-새-------다--------',\n",
       " '-현----대-----적-----',\n",
       " '-오----른----쪽------',\n",
       " '-과-------일--------',\n",
       " '-먹-----히---다------',\n",
       " '-병----------다-----',\n",
       " '-각-------학--------',\n",
       " '-스-------키--------',\n",
       " '-괴-로--위---하---다---',\n",
       " '-둘-------째--------',\n",
       " '-분-------야--------',\n",
       " '-어--------른-------',\n",
       " '-내--------씨-------',\n",
       " '-아----프------다----',\n",
       " '-듯---싶----다-------',\n",
       " '-튀-----기---다------',\n",
       " '-강----력-------히---',\n",
       " '-시------인---------',\n",
       " '-해----롭롭---다------',\n",
       " '-재--활----용---품----',\n",
       " '-숴----------------',\n",
       " '-위-------층--------',\n",
       " '-감----싸-----다-----',\n",
       " '-마-------마--------',\n",
       " '-뛰-------다다-------',\n",
       " '-재--미----있----다---',\n",
       " '-가--------치-------',\n",
       " '-국----가-----적-----',\n",
       " '-사---계------절-----',\n",
       " '-파----란------식----',\n",
       " '-안--녕녕---하----다---',\n",
       " '-자--------리-------',\n",
       " '-차------싸---------',\n",
       " '-만-족---스--럽---다---',\n",
       " '-정-------문--------',\n",
       " '-국-------적--------',\n",
       " '-데----이----블------',\n",
       " '-나-------무--------',\n",
       " '-아---무--렇----다----',\n",
       " '-발-------발--------',\n",
       " '-형-----사----------',\n",
       " '-민------주---------',\n",
       " '-특--정---하---다-----',\n",
       " '-뇌------다---------',\n",
       " '-실----젖-----------',\n",
       " '-배----우----자자-----',\n",
       " '-발-------리리-------',\n",
       " '-가-------방--------',\n",
       " '-아-------냐--------',\n",
       " '-이------번---------',\n",
       " '-초----상-----화-----',\n",
       " '-백--------성-------',\n",
       " '-트----리---다-------',\n",
       " '------------------',\n",
       " '-살-------림--------',\n",
       " '-아---울------러-----',\n",
       " '-직---장장-----인-----',\n",
       " '-의--논--하-----다----',\n",
       " '-또------탄---------',\n",
       " '-단-------맛--------',\n",
       " '-승---용-----차------',\n",
       " '-풀-----다----------',\n",
       " '-희-----다----------',\n",
       " '-기---초---하---다----',\n",
       " '-호----------------',\n",
       " '-소-----지----품-----',\n",
       " '-귀----찮----다------',\n",
       " '-설-------탕--------',\n",
       " '-한-----라----산-----',\n",
       " '-장-------관--------',\n",
       " '-부------상---------',\n",
       " '-귀--중---하---다-----',\n",
       " '-비---워----크-리-----',\n",
       " '-이--사------장------',\n",
       " '-소-----유----------',\n",
       " '-계-------란--------',\n",
       " '-옛-날---이---야---기--',\n",
       " '-번-------호--------',\n",
       " '-연-----습----------',\n",
       " '-깊----숙-----이-----',\n",
       " '-택--------시-------',\n",
       " '-굶-------다--------',\n",
       " '-지------출---------',\n",
       " '-젖----------------',\n",
       " '-돌------물---------',\n",
       " '-그----러----나------',\n",
       " '-노--래---하---다-----',\n",
       " '-실-----패----------',\n",
       " '-이-야---기--하---다---',\n",
       " '-대----------------',\n",
       " '-중------순---------',\n",
       " '-이--해----관---계----',\n",
       " '-틀----------------',\n",
       " '-어----머-----님-----',\n",
       " '-산--부----인---과----',\n",
       " '-문--학------적------',\n",
       " '-선-------생--------',\n",
       " '-완-------성성-------',\n",
       " '-한----밤-----중-----',\n",
       " '-뛰-어---다---니---다--',\n",
       " '-기-------원--------',\n",
       " '-내--려----오---다다---',\n",
       " '-전------환---------',\n",
       " '-수----------------',\n",
       " '-운---동------장-----',\n",
       " '-월-----요----일-----',\n",
       " '-복------도---------',\n",
       " '-순--하하-----다------',\n",
       " '-시----면-----트-----',\n",
       " '-싶-------다--------',\n",
       " '-봐----------------',\n",
       " '-깔-----리----다-----',\n",
       " '-반-------인--------',\n",
       " '-제--공----하----다---',\n",
       " '-시----------------',\n",
       " '-연-------필--------',\n",
       " '-비----디-----오-----',\n",
       " '-남-------쪽--------',\n",
       " '-산--------책-------',\n",
       " '-씨----------------',\n",
       " '-제--외----되----다---',\n",
       " '-온-------원--------',\n",
       " '-부-------엌--------',\n",
       " '-국-------가--------',\n",
       " '-아--------들-------',\n",
       " '-걱--------정-------',\n",
       " '-만-------족--------',\n",
       " '-엄--침----나---다----',\n",
       " '-않------다---------',\n",
       " '-간-------혹--------',\n",
       " '-부---------다------',\n",
       " '-법------률---------',\n",
       " '-형-------편--------',\n",
       " '-가---리---키---다----',\n",
       " '-외----다다----------',\n",
       " '-물------기---------',\n",
       " '-부------인---------',\n",
       " '-단--------지-------',\n",
       " '-하-------루--------',\n",
       " '-신------고---------',\n",
       " '-잡---히히-----다-----',\n",
       " '-모----퉁----이------',\n",
       " '-운------문---------',\n",
       " '-이--해---하----다----',\n",
       " '-오----------------',\n",
       " '-쓰------다---------',\n",
       " '-반--------음-------',\n",
       " '-찌-------찌찌-------',\n",
       " '-가------음---------',\n",
       " '-종-------일--------',\n",
       " '-쥐-------일--------',\n",
       " '-가---득---하---다----',\n",
       " '-배----우----다------',\n",
       " '-시----------------',\n",
       " '-받-------침--------',\n",
       " '-구------십십--------',\n",
       " '-한--정----되---다----',\n",
       " '-기---울---이---다----',\n",
       " '-인------제---------',\n",
       " '-건-----강----------',\n",
       " '-시------월---------',\n",
       " '-채----------------',\n",
       " '-충-------고--------',\n",
       " '-회----의-----------',\n",
       " '-장-------소--------',\n",
       " '-씨----------------',\n",
       " '-내-------년--------',\n",
       " '-그----------------',\n",
       " '-데--려----가---다----',\n",
       " '-뵈----------------',\n",
       " '-엉----터------리----',\n",
       " '-라----이-----터-----',\n",
       " '-등-장----하----다----',\n",
       " '-싫------다---------',\n",
       " '-키----우-----다-----',\n",
       " '-체----하-----다-----',\n",
       " '-그----래-----서-----',\n",
       " '-오---로-----지------',\n",
       " '-봐---------성------',\n",
       " '-이--것----저----것---',\n",
       " '-아-------직--------',\n",
       " '-마-------리--------',\n",
       " '-동-------안--------',\n",
       " '-잡--아----먹---다----',\n",
       " '-변----------------',\n",
       " '-본-----사----------',\n",
       " '-병----들---다-------',\n",
       " '-숙----이----다------',\n",
       " '-행--------위-------',\n",
       " '-의------자---------',\n",
       " '-세-----세----------',\n",
       " '-이-------동--------',\n",
       " '-치------다---------',\n",
       " '-끝------내---------',\n",
       " '-안----되----다------',\n",
       " '---------력--------',\n",
       " '------------------',\n",
       " '-로---터------리-----',\n",
       " '-불-완---전--하----다--',\n",
       " '-되---벗----다다------',\n",
       " '-바-------탕--------',\n",
       " '-입-학----하----다----',\n",
       " '-노------인---------',\n",
       " '-조--용----하---다----',\n",
       " '-우------유---------',\n",
       " '-곧----------------',\n",
       " '-내-------나--------',\n",
       " '-대-------답--------',\n",
       " '-캐-----나----닥-----',\n",
       " '-금---지---하----다---',\n",
       " '-이----웃-----집-----',\n",
       " '-전-------구--------',\n",
       " '-어--떠----하---다----',\n",
       " '-믿-----음----------',\n",
       " '-윅-------낙--------',\n",
       " '-어----리----다------',\n",
       " '-해---------차------',\n",
       " '-거--배배---하---다다---',\n",
       " '-붙-잡----히----다----',\n",
       " '-언------니---------',\n",
       " '-예--------산-------',\n",
       " '-무----용----가------',\n",
       " '-돼-------지--------',\n",
       " '-재----빨------리----',\n",
       " '-흔--들----리---다----',\n",
       " '-동------안---------',\n",
       " '-잔----------------',\n",
       " '-운-------명--------',\n",
       " '-이------쪽---------',\n",
       " '-초------점---------',\n",
       " '-기--------후-------',\n",
       " '-지---우우-----개-----',\n",
       " '-통-----장----------',\n",
       " '-가--------치-------',\n",
       " '-흘러---내---리---다---',\n",
       " '-센-------터--------',\n",
       " '-정---류------장-----',\n",
       " '-인---터-----뷰------',\n",
       " '-보----------------',\n",
       " '-숙-------녀녀-------',\n",
       " '-일--어----나---다----',\n",
       " '-연-----세----------',\n",
       " '-바----------다-----',\n",
       " '-파-------딱--------',\n",
       " '-여-------섯--------',\n",
       " '-날--아----오----다---',\n",
       " '-정--리----되---다----',\n",
       " '-분---비----다-------',\n",
       " '-해----내-----다-----',\n",
       " '-점-------점--------',\n",
       " '-레---스---토---량----',\n",
       " '-임-------시--------',\n",
       " '-비----------------',\n",
       " '-스-------기--------',\n",
       " '-경-------필--------',\n",
       " '-사-------월--------',\n",
       " '-승----용-----차-----',\n",
       " '-꼼-------짝--------',\n",
       " '-각-------국--------',\n",
       " '-비----행------기----',\n",
       " '-지------것---------',\n",
       " '-미-------혼--------',\n",
       " '-학-------점--------',\n",
       " '-시---작---되---다----',\n",
       " '-수-------수--------',\n",
       " '-허-------리--------',\n",
       " '-경---제-----학------',\n",
       " '---------다--------',\n",
       " '-네----------------',\n",
       " '-경-------치--------',\n",
       " '-늙------다---------',\n",
       " '-백-----호-----정----',\n",
       " '-어-------제--------',\n",
       " '-씻-------다--------',\n",
       " '-볼-----일----------',\n",
       " '-손---잡-----다------',\n",
       " '----------심-------',\n",
       " '-쓰-------다--------',\n",
       " '-안-------다--------',\n",
       " '-벼--벽----하---다----',\n",
       " '-의--지--하하---다-----',\n",
       " '-해----------------',\n",
       " '-섭--섭---하----다----',\n",
       " '-따--뜻----하---다다---',\n",
       " '-쇼------핑---------',\n",
       " '-경------갈----녀----',\n",
       " '-참--------외-------',\n",
       " '-안----되-----다-----',\n",
       " '-듣-----다다---------',\n",
       " '-비-------히--------',\n",
       " '-음----식-----점-----',\n",
       " '-환---하------다-----',\n",
       " '-대-------낮--------',\n",
       " '-넷------째---------',\n",
       " '-예----------------',\n",
       " '-영-------화--------',\n",
       " '-라----이----벌------',\n",
       " '-씨----------------',\n",
       " '-몇-------명--------',\n",
       " '-벌--이-----다-------',\n",
       " '-선--------구-------',\n",
       " '-카-----폐----------',\n",
       " '-운-------동--------',\n",
       " '-찍-------픽--------',\n",
       " '-영--화화---배----우---',\n",
       " '-진------급---------',\n",
       " '-필------자자--------',\n",
       " '-적--용--하----다-----',\n",
       " '-입------다---------',\n",
       " '-많-------다--------',\n",
       " '-키--다----리---다----',\n",
       " '-방-------면--------',\n",
       " '-반-------면--------',\n",
       " '-수---영------장-----',\n",
       " '-제--의---하하---다----',\n",
       " '-불----리----다------',\n",
       " '-불---교------기-----',\n",
       " '-돌----리----다------',\n",
       " '-상-------처--------',\n",
       " '-이-------념--------',\n",
       " '-바----------------',\n",
       " '-환----하-----다-----',\n",
       " '-보-----도도---------',\n",
       " '-웃------다---------',\n",
       " '-배----------------',\n",
       " '-도------구---------',\n",
       " '-날-------짜--------',\n",
       " '-비----------------',\n",
       " '-중-------국--------',\n",
       " '-별------일---------',\n",
       " '-쇠-------금--------',\n",
       " '-심--각----하----다---',\n",
       " '-변---갛----다다------',\n",
       " '-결----과-----적-----',\n",
       " '-타-------다--------',\n",
       " '-약--------정-------',\n",
       " '-나------다---------',\n",
       " '-배----------------',\n",
       " '-받-아---들---이--다---',\n",
       " '-바---이---러----스---',\n",
       " '-이-------달---다----',\n",
       " '-안-------다--------',\n",
       " '-치----------------',\n",
       " '-이-------번--------',\n",
       " '-모-----양----------',\n",
       " '-여--행----하----다---',\n",
       " '-레--------몬-------',\n",
       " '-팩-------찍--------',\n",
       " '-세----상-----에-----',\n",
       " '-따-------다--------',\n",
       " '-힘----들----다------',\n",
       " '-개---선---하---다----',\n",
       " '-교----과-----서-----',\n",
       " '-있-----다----------',\n",
       " '-매------주---------',\n",
       " '-급---속------히-----',\n",
       " '-사-------위--------',\n",
       " '-날-------다--------',\n",
       " '-노-----트----------',\n",
       " '-사-------이--------',\n",
       " '-미------혼---------',\n",
       " '-설------문---------',\n",
       " '-늦-----다----------',\n",
       " '-여--보----세---요----',\n",
       " '-가--------정-------',\n",
       " '-산--------책-------',\n",
       " '-흘-러---나---오---다--',\n",
       " '------------------',\n",
       " '-방-------지--------',\n",
       " '-만-----녀----다-----',\n",
       " '-약--------속-------',\n",
       " '-요-------일--------',\n",
       " '-탄-------두--------',\n",
       " '-생--활----수----준---',\n",
       " '-형-------식--------',\n",
       " '-배----------------',\n",
       " '-절------대---------',\n",
       " '-시----------------',\n",
       " '-그-------쪽--------',\n",
       " '-쓰----레-----기-----',\n",
       " '-운--------동-------',\n",
       " '-지----난----해------',\n",
       " '-한---마------디-----',\n",
       " '-교------외---------',\n",
       " '-밟------다다--------',\n",
       " '-경-------력--------',\n",
       " '-박------다---------',\n",
       " '-사---용----되---다---',\n",
       " '-주--요----하----다---',\n",
       " '-발--------끝-------',\n",
       " '-치-------즈--------',\n",
       " '-이------번---------',\n",
       " '-건------물---------',\n",
       " '-배-------꼽--------',\n",
       " '-통--------신-------',\n",
       " '-접------다---------',\n",
       " '-얄----밉-----다-----',\n",
       " '-임-----신----부-----',\n",
       " '-묻---히-----다------',\n",
       " '-불-과----하----다----',\n",
       " '-추-----다----------',\n",
       " '-고---르-----다다-----',\n",
       " '-방-------필--------',\n",
       " '-친------번---------',\n",
       " '-게----------------',\n",
       " '-뒤----------------',\n",
       " '-접----하-----다-----',\n",
       " '-앓------다---------',\n",
       " '-규----칙-----적-----',\n",
       " '-마-------흔--------',\n",
       " '-셋----------------',\n",
       " '-자--------식-------',\n",
       " '-신------고고--------',\n",
       " '-서-----미----스-----',\n",
       " '-시----------------',\n",
       " '-요-------리--------',\n",
       " '-운-------동--------',\n",
       " '-팔-------다--------',\n",
       " '-막----히-----다-----',\n",
       " '-패---------문------',\n",
       " '-사-------월--------',\n",
       " '-나----오-----다-----',\n",
       " '-여----------------',\n",
       " '-동----대-----문-----',\n",
       " '-망--설----이----다---',\n",
       " '-선-------선--------',\n",
       " '-십---일-----월------',\n",
       " '-그-------릇--------',\n",
       " '-되-돌--아---보---다---',\n",
       " '-펴------다---------',\n",
       " '-찌-------씨--------',\n",
       " '-조-------기--------',\n",
       " '-어----린-----이-----',\n",
       " '-낫------매---------',\n",
       " '-오-------늘--------',\n",
       " '-핑-------적--------',\n",
       " '-부------엌---------',\n",
       " '-셋-------째--------',\n",
       " '-둘--러----보---다----',\n",
       " '-태---어---나---다----',\n",
       " '-마--땅----하---다----',\n",
       " '-생-활----수수---준----',\n",
       " '-칠--------십-------',\n",
       " '-연-락----하----다----',\n",
       " '-씨----------------',\n",
       " '-입--원---하----다----',\n",
       " '-우-------승--------',\n",
       " '-어-려---워---지---다--',\n",
       " '-흔-들---리----다-----',\n",
       " '-출-------판--------',\n",
       " '-사-------탕--------',\n",
       " '-계-------절절-------',\n",
       " '-차------다---------',\n",
       " '-떡----볶-----이-----',\n",
       " '-짜-------까--------',\n",
       " '-채----------------',\n",
       " '-시-------간--------',\n",
       " '-마-------침침-------',\n",
       " '-교-------환--------',\n",
       " '-제--의----하---다----',\n",
       " '-소--화----하----다---',\n",
       " '-드--러----나----다---',\n",
       " '-앓-----다----------',\n",
       " '-----하-----다------',\n",
       " '-호------텔---------',\n",
       " '-할-------인--------',\n",
       " '-셋------째째--------',\n",
       " '-교-------내--------',\n",
       " '-끊--임----없---다----',\n",
       " '-쉽-------다--------',\n",
       " '-살---피-----다------',\n",
       " '-서-------점--------',\n",
       " '-환--------갑-------',\n",
       " '-재--------로-------',\n",
       " '-넘------다---------',\n",
       " '-소---개---하---다----',\n",
       " '-코------끝---------',\n",
       " '-만-------흔--------',\n",
       " '-경------복---궁-----',\n",
       " '-빠-------짜까-------',\n",
       " '------------------',\n",
       " '-예-------절--------',\n",
       " '-경-----기----도-----',\n",
       " '-국-------왕--------',\n",
       " '-비------중---------',\n",
       " '-엽-------서서-------',\n",
       " '-과-------일--------',\n",
       " '-회------견---------',\n",
       " '-사----------------',\n",
       " '-엉----덩-----이-----',\n",
       " '-뒤----------------',\n",
       " '-복----숭---아-------',\n",
       " '-의-------욕--------',\n",
       " '-어--씩---하----다----',\n",
       " '-불------다---------',\n",
       " '-부--------산-------',\n",
       " '-어-------툼--------',\n",
       " '-약----------------',\n",
       " '-꽃-------씨--------',\n",
       " '-발----------------',\n",
       " '-정--하------다------',\n",
       " '-적-----다----------',\n",
       " '-탬----------------',\n",
       " '-스----스-----로-----',\n",
       " '-할----머-----늬-----',\n",
       " '-교-------장--------',\n",
       " '-됫----------------',\n",
       " '-색----------------',\n",
       " '-지----도도----자-----',\n",
       " '-차----------------',\n",
       " '-뛰어--나----오---다---',\n",
       " '-많-------이이-------',\n",
       " '-결-------과--------',\n",
       " '-배----------------',\n",
       " '-대-------충충-------',\n",
       " '-신-----천----서-----',\n",
       " '-전-------선--------',\n",
       " '-비-------중--------',\n",
       " '-자--연----현----상---',\n",
       " '-시----멘-----트-----',\n",
       " '-주-----말----------',\n",
       " '-뒷---골------목-----',\n",
       " ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51cef1b-f593-4b88-b722-d54c98862297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c90e4ec7-2d64-4b83-bce1-4be2058441cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>./train/TRAIN_00000.png</td>\n",
       "      <td>빨간색</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>./train/TRAIN_00001.png</td>\n",
       "      <td>머</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>./train/TRAIN_00002.png</td>\n",
       "      <td>차차</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>./train/TRAIN_00003.png</td>\n",
       "      <td>써</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>./train/TRAIN_00004.png</td>\n",
       "      <td>놓치다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                 img_path label\n",
       "0  TRAIN_00000  ./train/TRAIN_00000.png   빨간색\n",
       "1  TRAIN_00001  ./train/TRAIN_00001.png     머\n",
       "2  TRAIN_00002  ./train/TRAIN_00002.png    차차\n",
       "3  TRAIN_00003  ./train/TRAIN_00003.png     써\n",
       "4  TRAIN_00004  ./train/TRAIN_00004.png   놓치다"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bdfc5-56b9-455f-ad89-38f931007149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
